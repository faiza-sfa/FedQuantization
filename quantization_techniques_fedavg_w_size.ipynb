{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Apr 28 07:08:16 2024\n",
        "\n",
        "\n",
        "@author: Faiza\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import struct\n",
        "from typing import Tuple\n",
        "from bitarray import bitarray\n",
        "# from src.compressors.compressor import Compressor\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from load_dataset import Dataset\n",
        "import os\n",
        "\n",
        "import time\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "import cv2\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "import math\n",
        "# from src.compressors.compressor import Compressor\n",
        "import struct\n",
        "\n",
        "\n",
        "from bitarray import bitarray\n",
        "from bitarray.util import int2ba, ba2int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Compressor(ABC):\n",
        "    @abstractmethod\n",
        "    def encode(self, array: np.ndarray) -> bytes:\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def decode(self, array: bytes) -> np.ndarray:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QSGD\n",
            "Before Quantization:  [9.602516  4.5155435 1.8452946 2.3192837 7.6018715 9.087472  4.4516053\n",
            " 8.550069  4.7978096 9.133822  9.016326  3.9488978 4.72986   9.617058\n",
            " 7.335939  4.0205336]\n",
            "Encoded b'$\\x00\\x00\\x00\\xb4\\n\\xdaA\\xb2\\xcadR\\x00\\x00\\x00\\x00'\n",
            "After Quantization:  [13.627613  0.        0.        0.        0.        0.       13.627613\n",
            "  0.        0.       13.627613  0.        0.       13.627613  0.\n",
            " 13.627613  0.      ]\n",
            "52.88461538461539% smaller \n"
          ]
        }
      ],
      "source": [
        "from julia.api import Julia\n",
        "jl = Julia(compiled_modules=False)\n",
        "from julia import Main\n",
        "Main.eval(\"using Random; Random.seed!(0)\")\n",
        "Main.include(\"src/compressors/qsgd.jl\")\n",
        "\n",
        "\n",
        "class QSGD(Compressor):\n",
        "    def __init__(self, s: int, zero_rle: bool, type=None):\n",
        "        self.type = type\n",
        "        if self.type == \"LFL\":\n",
        "            self.s = 2\n",
        "        else:\n",
        "            self.s = s\n",
        "        self.zero_rle = zero_rle\n",
        "\n",
        "    # format:    | length | norm | s(1) | sign(1) | s(2) | ... | sign(n) | \n",
        "    # (no 0-rle) | 32     | 32   | ?    | 1       | ?    | ... | 1       |\n",
        "    # format:    | length | norm | n_zeros | s(1) | sign(1) | n_zeros | s(2) | ... | sign(n) |\n",
        "    # (0-rle)    | 32     | 32   | ?       | ?    | 1       | ?       | ?    | ... | 1       |\n",
        "    def encode(self, array: np.ndarray, seed=None, cid=\":-)\", client_name=\":-))\") -> bytes:\n",
        "        assert len(array.shape) == 1\n",
        "        assert array.dtype == np.float32\n",
        "        result = Main.encode_qsgd(array, self.s, self.type, seed, cid, client_name)\n",
        "        return bytes(result)\n",
        "\n",
        "    def decode(self, array: bytes, use_lo_quant=False) -> np.ndarray:\n",
        "        result = Main.decode_qsgd(array, self.s, self.type, use_lo_quant)\n",
        "        return result\n",
        "    \n",
        "#%% testing qsgd quantization\n",
        "\n",
        "print(\"QSGD\")\n",
        "arr = list(np.random.rand(1,16).flatten()*10)\n",
        "arr = np.array(arr, dtype = np.float32)\n",
        "print(\"Before Quantization: \", arr)\n",
        "cmp = QSGD(2,True)\n",
        "enc = cmp.encode(arr)\n",
        "print(\"Encoded\",enc)\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GZip\n",
            "Before Quantization:  [8.392763  1.8799031 0.7727104 1.8461773 7.3292036 6.8138433 3.4568703\n",
            " 3.5700812 7.6936374 1.4033586 2.7966292 3.9259048 6.7647533 6.18365\n",
            " 9.893251  7.9981265]\n",
            "Encoded b'\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\ncd\\x00\\x01F(bd\\xc8\\xdd\\xb2\\xc3\\x11\\x00\\xe2a\\x19\\x17\\x14\\x00\\x00\\x00'\n",
            "After Quantization:  [23.088099  0.        0.        0.        0.        0.       23.088099\n",
            "  0.        0.       23.088099  0.        0.       23.088099  0.\n",
            " 23.088099  0.      ]\n",
            "37.5% smaller \n"
          ]
        }
      ],
      "source": [
        "#%%SETTING UP Julia and gzip Class\n",
        "from julia.api import Julia\n",
        "jl = Julia(compiled_modules=False)\n",
        "from julia import Main\n",
        "Main.eval(\"using Random; Random.seed!(0)\")\n",
        "Main.include(\"src/compressors/gzip.jl\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GZip(Compressor):\n",
        "    def __init__(self, s: int):\n",
        "        self.s = s\n",
        "\n",
        "    def encode(self, array: np.ndarray, seed=None, cid=\":-)\", client_name=\":-))\") -> bytes:\n",
        "        assert len(array.shape) == 1\n",
        "        assert array.dtype == np.float32\n",
        "        result = Main.encode_gzip(array, self.s, seed)\n",
        "        return bytes(result)\n",
        "\n",
        "    def decode(self, array: bytes, use_lo_quant=False) -> np.ndarray:\n",
        "        result = Main.decode_gzip(array, self.s)\n",
        "        return result\n",
        "\n",
        "\n",
        "#%% testing gzip quantization\n",
        "print(\"GZip\")\n",
        "arr = list(np.random.rand(1,16).flatten()*10)\n",
        "arr = np.array(arr, dtype = np.float32)\n",
        "print(\"Before Quantization: \", arr)\n",
        "cmp = GZip(1)\n",
        "enc = cmp.encode(arr)\n",
        "print(\"Encoded\",enc)\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\FedPAQ_env\\lib\\site-packages\\julia\\juliainfo.py:93: UserWarning: julia warned:\n",
            "The latest version of Julia in the `release` channel is 1.10.3+0.x64.w64.mingw32. You currently have `1.10.2+0.x64.w64.mingw32` installed. Run:\n",
            "\n",
            "  juliaup update\n",
            "\n",
            "in your terminal shell to install Julia 1.10.3+0.x64.w64.mingw32 and update the `release` channel to that version.\n",
            "  warnings.warn(\"{} warned:\\n{}\".format(julia, stderr))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP8\n",
            "Before Quantization:  [4.126472  2.749015  3.8986733 9.317252  2.5157616 9.321195  8.882262\n",
            " 2.9293163 9.324964  1.5345355 2.1583395 1.4662943 9.918267  6.0527186\n",
            " 8.331317  7.22466  ]\n",
            "Encoded b'EBCIAHIAI?@>IFHG'\n",
            "After Quantization:  [ 5.    3.    3.5  10.    2.5   8.   10.    2.5  10.    1.75  2.    1.5\n",
            " 10.    6.    8.    7.  ]\n",
            "52.88461538461539% smaller \n"
          ]
        }
      ],
      "source": [
        "#%%FP8\n",
        "\n",
        "# print(\"Starting up Julia.\")\n",
        "from julia.api import Julia\n",
        "jl = Julia(compiled_modules=False)\n",
        "from julia import Main\n",
        "Main.eval(\"using Random; Random.seed!(0)\")\n",
        "Main.include(\"src/compressors/fp8.jl\")\n",
        "# print(\"Finished starting up Julia.\")\n",
        "\n",
        "FP8_FORMAT = (1, 5, 2)\n",
        "FP32_FORMAT = (1, 8, 23)\n",
        "\n",
        "def get_emax(format):\n",
        "    return (2**(format[1]-1)) - 1\n",
        "\n",
        "def get_emin(format):\n",
        "    return 1 - get_emax(format)\n",
        "\n",
        "\n",
        "class FP8(Compressor):\n",
        "    def __init__(self):\n",
        "        self.fp8s_repr_in_fp32 = []\n",
        "        self.fp8s = []\n",
        "        self.s = -1\n",
        "        # negative values before positive values.\n",
        "        def insert(num):\n",
        "            byte = struct.pack('>B', num)\n",
        "            [num] = self.decode(byte)\n",
        "            if not np.isnan(num):\n",
        "                self.fp8s.append(byte)\n",
        "                self.fp8s_repr_in_fp32.append(num)\n",
        "                bits = bitarray()\n",
        "                bits.frombytes(byte)\n",
        "        for i in list(reversed(range(128, 253))) + list(range(0, 128)):\n",
        "            insert(i)\n",
        "        self.fp8s_repr_in_fp32 = np.array(self.fp8s_repr_in_fp32).astype(np.float32)\n",
        "\n",
        "    def get_fp8_neighbors(self, f: np.float32) -> Tuple[bytes, bytes]:\n",
        "        idx_high = np.searchsorted(self.fp8s_repr_in_fp32, f, side='right')\n",
        "        idx_low = idx_high - 1\n",
        "        if idx_high == len(self.fp8s_repr_in_fp32):\n",
        "            idx_high -= 1\n",
        "        return self.fp8s[idx_low], self.fp8s[idx_high], self.fp8s_repr_in_fp32[idx_low], self.fp8s_repr_in_fp32[idx_high]\n",
        "\n",
        "    def encode(self, array: np.ndarray, seed=None, cid=\":-)\", client_name=\":-))\") -> bytes:\n",
        "        assert len(array.shape) == 1\n",
        "        assert array.dtype == np.float32\n",
        "        result = Main.encode_fp8(array, self.fp8s_repr_in_fp32, self.fp8s, seed)\n",
        "        return bytes(result)\n",
        "    \n",
        "    def decode(self, array: bytes, use_lo_quant=False) -> np.ndarray:\n",
        "        result = Main.decode_fp8(array)\n",
        "        return result\n",
        "#%% testing fp32 quantization\n",
        "print(\"FP8\")\n",
        "arr = list(np.random.rand(1,16).flatten()*10)\n",
        "arr = np.array(arr, dtype = np.float32)\n",
        "print(\"Before Quantization: \", arr)\n",
        "cmp = FP8()\n",
        "enc = cmp.encode(arr)\n",
        "print(\"Encoded\",enc)\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5.4346137  0.39908051 0.54436535 7.3529234  3.7397492  3.752178\n",
            " 0.53688484 0.20883559 8.684521   9.365884   4.9378195  0.7419795\n",
            " 5.63708    1.8902718  1.9684689  0.8976962 ]\n",
            "Encoded [37399  1361  2401 51128 25269 25358  2347     0 60658 65535 33844  3815\n",
            " 38848 12033 12593  4930]\n",
            "After Quantization:  [5.43450808 0.39900485 0.54432155 7.3528283  3.7396123  3.75204806\n",
            " 0.53677626 0.20883559 8.68443232 9.36588383 4.93777647 0.74189638\n",
            " 5.63697338 1.89017779 1.96842525 0.89769265]\n",
            "<class 'numpy.uint16'> 41.37931034482759% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  7.12347644835608e-09\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "class customCompression():\n",
        "    def __init__(self, quantization_type = np.uint32):\n",
        "        self.quantization_type = quantization_type\n",
        "        #quantization_level from 0 to 100\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self,data):\n",
        "        # data = data.astype(int)\n",
        "        self.xp = [data.min(), data.max()]\n",
        "        min = np.iinfo(self.quantization_type).min\n",
        "        max = np.iinfo(self.quantization_type).max\n",
        "\n",
        "        self.fp = [min, max]\n",
        "        enc = np.interp(data, self.xp, self.fp)\n",
        "        enc = enc.astype(self.quantization_type)\n",
        "        # encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quantization_level] \n",
        "        # result, encimg = cv2.imencode('.jpg', img, encode_param)\n",
        "\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        dec = np.interp(enc, self.fp, self.xp)\n",
        "        return dec\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# data = np.random.rand(1,16)\n",
        "\n",
        "data = list(np.random.rand(1,16).flatten()*10)\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customCompression(data_type)\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "552\n",
            "552\n",
            "202\n"
          ]
        }
      ],
      "source": [
        "print(sys.getsizeof(data))\n",
        "print(sys.getsizeof(dec))\n",
        "\n",
        "print(sys.getsizeof(enc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "unint8\n",
        "36.231884057971016% smaller \n",
        "enc shape:  (2, 1, 5, 5)\n",
        "dec shape:  (2, 1, 5, 5)\n",
        "mse:  4.056377131919988e-06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class customFreq():\n",
        "    def __init__(self, precision_levels = [8, 4, 2]):\n",
        "        self.precision_levels = precision_levels\n",
        "        #quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(dct(x, axis=-1, norm='ortho'), axis=-2, norm='ortho')\n",
        "\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(idct(x, axis=-1, norm='ortho'), axis=-2, norm='ortho')\n",
        "\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        \n",
        "        # Calculate importance as the magnitude of the frequency components\n",
        "        importance = np.abs(weights_f)\n",
        "        \n",
        "        # Assign precision based on importance\n",
        "        mean_importance = np.mean(importance)\n",
        "        if mean_importance > 0.1:\n",
        "            precision = self.precision_levels[0]  # 8-bit\n",
        "        elif mean_importance > 0.01:\n",
        "            precision = self.precision_levels[1]  # 4-bit\n",
        "        else:\n",
        "            precision = self.precision_levels[2]  # 2-bit\n",
        "        \n",
        "        # Quantize frequency components\n",
        "        enc = self.quantize(weights_f, precision)\n",
        "        enc = enc.astype(np.float16)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        weights_quantized = self.inverse_dct_transform(enc)\n",
        "        return weights_quantized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% PAQ with Compression\n",
        "\n",
        "\n",
        "class MNIST_PAQ_COMP:\n",
        "\tdef __init__(self, cmp, filename=\"saved_models\", number_of_clients=1, aggregate_epochs=10, local_epochs=5, precision=7, r=1.0):\n",
        "\t\tself.model = None\n",
        "\t\tself.criterion = torch.nn.CrossEntropyLoss()\n",
        "\t\tself.optimizer = None\n",
        "\t\tself.number_of_clients = number_of_clients\n",
        "\t\tself.aggregate_epochs = aggregate_epochs\n",
        "\t\tself.local_epochs = local_epochs\n",
        "\t\tself.precision = precision\n",
        "\t\tself.r = r\n",
        "\t\tself.filename = filename\n",
        "\t\tself.compression_ratio = 0\n",
        "\t\tself.cmp=cmp\n",
        "\n",
        "\tdef define_model(self):\n",
        "\t\tself.model = torch.nn.Sequential(\n",
        "\t\t\ttorch.nn.Conv2d(1, 2, kernel_size=5),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Conv2d(2, 4, kernel_size=7),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Flatten(),\n",
        "\t\t\ttorch.nn.Linear(1296, 512),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Linear(512, 128),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Linear(128, 32),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Linear(32, 10),\n",
        "\t\t\ttorch.nn.Softmax(dim=1),\n",
        "\t\t)\t\t\n",
        "\n",
        "\t# def get_weights(self, dtype=np.float32):\n",
        "        \n",
        "\t# \tprecision = self.precision \n",
        "\t# \tweights = []\n",
        "\t# \tstart_size = 0\n",
        "\t# \tend_size = 0\n",
        "        \n",
        "        \n",
        "\t# \tfor layer in self.model:\n",
        "\t\t\t\n",
        "\t# \t\ttry:\n",
        "\t# \t\t\tlayer_name = layer.__repr__\n",
        "\t# \t\t\tprint('Layer Name: ', layer_name)\n",
        "\t# \t\t\t# weights.append([np.around(layer.weight.detach().numpy().astype(dtype), decimals=precision), np.around(layer.bias.detach().numpy().astype(dtype), decimals=precision)])\n",
        "\n",
        "    #             # layer_name = layer.__class__name__\n",
        "    #             # if layer_name = 'Conv2d':\n",
        "    #             #     layer_np = layer.weight.detach().numpy().astype(dtype)\n",
        "    #             #     for filters in layer_np:\n",
        "    #             #         for channels in filters:\n",
        "    #             #             for inp_dim1 in channels:\n",
        "    #             #                 fp8.encode(inp_dim1)\n",
        "                \n",
        "\t\t\t\t\n",
        "\t# \t\t\tlayer_weights = layer.weight.detach().numpy().astype(dtype)\n",
        "\t# \t\t\tstart_size = start_size + sys.getsizeof(layer_weights)\n",
        "\t# \t\t\tlayer_bias = layer.bias.detach().numpy().astype(dtype)\n",
        "\t# \t\t\tlayer_weights_enc = self.cmp.encode(layer_weights.flatten())\n",
        "\t# \t\t\tlayer_bias_enc = self.cmp.encode(layer_bias.flatten())\n",
        "\t# \t\t\tdecoded_weights = self.cmp.decode(layer_weights_enc).reshape(layer_weights.shape)\n",
        "\t# \t\t\tend_size = end_size + sys.getsizeof(decoded_weights)\n",
        "\t# \t\t\tprint(\"decoded_weights.shape\",decoded_weights.shape)\n",
        "\t# \t\t\tdecoded_bias = self.cmp.decode(layer_bias_enc).reshape(layer_bias.shape)\n",
        "\t# \t\t\tweights.append([decoded_weights,decoded_bias])\n",
        "\t# \t\texcept:\n",
        "\t# \t\t\tcontinue\n",
        "\t# \tself.compression_ratio = ((start_size-end_size)/start_size)*100\n",
        "\t# \treturn np.array(weights),start_size,end_size\n",
        "\n",
        "\n",
        "\n",
        "\tdef get_weights_custom(self, dtype=np.float32):\n",
        "        \n",
        "\t\tprecision = self.precision \n",
        "\t\tweights = []\n",
        "\t\tstart_size = 0\n",
        "\t\tend_size = 0\n",
        "        \n",
        "        \n",
        "\t\tfor layer in self.model:\t\t\t\n",
        "\t\t\ttry:\n",
        "\t\t\t\tlayer_weights = layer.weight.detach().numpy().astype(dtype)\n",
        "\t\t\t\tstart_size = start_size + sys.getsizeof(layer_weights)\n",
        "\t\t\t\t# layer_weights_enc = self.cmp.encode(layer_weights)\n",
        "\t\t\t\tlayer_weights_enc = self.cmp.encode(layer_weights.flatten())\n",
        "\t\t\t\tdecoded_weights = self.cmp.decode(layer_weights_enc)\n",
        "\t\t\t\tdecoded_weights = decoded_weights.astype(dtype)\n",
        "\t\t\t\tdecoded_weights = decoded_weights.reshape(layer_weights.shape)\n",
        "\t\t\t\tend_size = end_size + sys.getsizeof(decoded_weights)\n",
        "\t\t\n",
        "\t\t\t\tlayer_bias = layer.bias.detach().numpy().astype(dtype)\n",
        "\t\t\t\t# layer_bias_enc = self.cmp.encode(layer_bias)\t\n",
        "\t\t\t\tlayer_bias_enc = self.cmp.encode(layer_bias.flatten())\t\t\t\n",
        "\t\t\t\tdecoded_bias = self.cmp.decode(layer_bias_enc)\n",
        "\t\t\t\tdecoded_bias = decoded_bias.astype(dtype)\n",
        "\t\t\t\tdecoded_bias = decoded_bias.reshape(layer_bias.shape)\n",
        "\t\t\t\tweights.append([decoded_weights,decoded_bias])\n",
        "\t\t\t\t\n",
        "\t\t\texcept:\n",
        "\t\t\t\tcontinue\n",
        "\t\tself.compression_ratio = ((start_size-end_size)/start_size)*100\n",
        "\t\treturn np.array(weights),start_size,end_size\n",
        "\n",
        "\n",
        "\tdef set_weights(self, weights):\n",
        "\t\tindex = 0\n",
        "\t\tfor layer_no, layer in enumerate(self.model):\n",
        "\t\t\ttry:\n",
        "\t\t\t\t_ = self.model[layer_no].weight\n",
        "\t\t\t\tself.model[layer_no].weight = torch.nn.Parameter(weights[index][0])\n",
        "\t\t\t\tself.model[layer_no].bias = torch.nn.Parameter(weights[index][1])\n",
        "\t\t\t\tindex += 1\n",
        "\t\t\texcept:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\tdef average_weights(self, all_weights):\n",
        "        \n",
        "\t\tall_weights = np.array(all_weights)\n",
        "\t\tall_weights = np.mean(all_weights, axis=0)\n",
        "\t\tall_weights = [[torch.from_numpy(i[0].astype(np.float32)), torch.from_numpy(i[1].astype(np.float32))] for i in all_weights]\n",
        "\t\treturn all_weights\n",
        "\n",
        "\tdef client_generator(self, train_x, train_y):\n",
        "\t\tnumber_of_clients = self.number_of_clients\n",
        "\t\tsize = train_y.shape[0]//number_of_clients\n",
        "\t\ttrain_x, train_y = train_x.numpy(), train_y.numpy()\n",
        "\t\ttrain_x = np.array([train_x[i:i+size] for i in range(0, len(train_x)-len(train_x)%size, size)])\n",
        "\t\ttrain_y = np.array([train_y[i:i+size] for i in range(0, len(train_y)-len(train_y)%size, size)])\n",
        "\t\ttrain_x = torch.from_numpy(train_x)\n",
        "\t\ttrain_y = torch.from_numpy(train_y)\n",
        "\t\treturn train_x, train_y\n",
        "\n",
        "\tdef single_client(self, dataset, weights, E):\n",
        "\t\tself.define_model()\n",
        "\t\tif weights is not None:\n",
        "\t\t\tself.set_weights(weights)\n",
        "\t\tself.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\t\tfor epoch in range(E):\n",
        "\t\t\trunning_loss = 0\n",
        "\t\t\tfor batch_x, target in zip(dataset['x'], dataset['y']):\n",
        "\t\t\t\toutput = self.model(batch_x)\n",
        "\t\t\t\tloss = self.criterion(output, target)\n",
        "\t\t\t\tself.optimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\tself.optimizer.step()\n",
        "\t\t\t\trunning_loss += loss.item()\n",
        "\t\t\trunning_loss /= len(dataset['y'])\n",
        "\t\t# weights,start_size,end_size = self.get_weights()\n",
        "\t\tweights,start_size,end_size = self.get_weights_custom()\n",
        "\t\t\n",
        "\t\treturn weights, running_loss, start_size,end_size\n",
        "\tdef test_aggregated_model(self, test_x, test_y, epoch):\n",
        "\t\tacc = 0\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor batch_x, batch_y in zip(test_x, test_y):\n",
        "\t\t\t\ty_pred = self.model(batch_x)\n",
        "\t\t\t\ty_pred = torch.argmax(y_pred, dim=1)\n",
        "\t\t\t\tacc += torch.sum(y_pred == batch_y)/y_pred.shape[0]\n",
        "\t\ttorch.save(self.model, \"./\"+self.filename+\"/model_epoch_\"+str(epoch+1)+\".pt\")\n",
        "\t\treturn (acc/test_x.shape[0])\n",
        "\t\t\t\n",
        "\n",
        "\tdef train_aggregator(self, datasets, datasets_test):\n",
        "\t\tlocal_epochs = self.local_epochs\n",
        "\t\taggregate_epochs = self.aggregate_epochs\n",
        "\t\tos.system('mkdir '+self.filename)\n",
        "\t\tE = local_epochs\n",
        "\t\taggregate_weights = None\n",
        "\t\tfor epoch in range(aggregate_epochs):\n",
        "\t\t\tall_weights = []\n",
        "\t\t\tclient = 0\n",
        "\t\t\trunning_loss = 0\n",
        "\t\t\tselections = np.arange(datasets['x'].shape[0])\n",
        "\t\t\tnp.random.shuffle(selections)\n",
        "\t\t\tselections = selections[:int(self.r*datasets['x'].shape[0])]\n",
        "\t\t\tclients = tqdm(zip(datasets['x'][selections], datasets['y'][selections]), total=selections.shape[0])\n",
        "\t\t\tfor dataset_x, dataset_y in clients:\n",
        "\t\t\t\tdataset = {'x':dataset_x, 'y':dataset_y}\n",
        "\t\t\t\tweights, loss, start_size,end_size = self.single_client(dataset, aggregate_weights, E)\n",
        "\t\t\t\trunning_loss += loss\n",
        "\t\t\t\tall_weights.append(weights)\n",
        "\t\t\t\tclient += 1\n",
        "\t\t\t\tclients.set_description(str({\"Epoch\":epoch+1,\"Loss\": round(running_loss/client, 5)}))\n",
        "\t\t\t\tclients.refresh()\n",
        "\t\t\taggregate_weights = self.average_weights(all_weights)\n",
        "\t\t\tagg_weight = self.set_weights(aggregate_weights)\n",
        "\t\t\ttest_acc = self.test_aggregated_model(datasets_test['x'], datasets_test['y'], epoch)\n",
        "\t\t\tprint(\"Test Accuracy:\", round(test_acc.item(), 5))\n",
        "\t\t\tprint(\"Compression Ratio \", self.compression_ratio)\n",
        "\t\t\t# print()\n",
        "\t\t\tprint(f'start_size: {start_size/1024},end_size: {end_size}')\n",
        "\t\t\tclients.close()\n",
        "\t\treturn agg_weight\n",
        "\n",
        "\n",
        "\n",
        "#%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/164 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "number_of_clients = 328\n",
        "aggregate_epochs = 10\n",
        "local_epochs = 3\n",
        "r = 0.5\n",
        "current_time = datetime.now().time()\n",
        "epoch_time = time.time()\n",
        "tic = time.time()\n",
        "filename = f\"saved_models_{epoch_time}\"\n",
        "train_x, train_y, test_x, test_y = Dataset().load_csv()\n",
        "# cmp = customCompression(np.uint8)\n",
        "cmp = customFreq()\n",
        "\n",
        "# cmp = FP8()\n",
        "# cmp = QSGD(2,True)\n",
        "m_8 = MNIST_PAQ_COMP(cmp = cmp, filename=filename, r=r, number_of_clients=number_of_clients, aggregate_epochs=aggregate_epochs, local_epochs=local_epochs)\n",
        "train_x, train_y = m_8.client_generator(train_x, train_y)\n",
        "agg_weights = m_8.train_aggregator({'x':train_x, 'y':train_y}, {'x':test_x, 'y':test_y})\n",
        "print(\"Time Taken: \", time.time()-tic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer_name <bound method Module.__repr__ of Conv2d(1, 2, kernel_size=(5, 5), stride=(1, 1))>\n",
            "2\n",
            "torch.Size([2, 1, 5, 5])\n",
            "Parameter containing:\n",
            "tensor([[[[-0.0987,  0.0324, -0.0006,  0.1326, -0.1166],\n",
            "          [ 0.0617, -0.1794,  0.1544, -0.1024,  0.1977],\n",
            "          [ 0.1142, -0.1490,  0.1352,  0.0402, -0.1834],\n",
            "          [-0.0960, -0.0115, -0.1335,  0.1792,  0.1388],\n",
            "          [ 0.0792,  0.0264,  0.1178, -0.1143, -0.1608]]],\n",
            "\n",
            "\n",
            "        [[[-0.0115,  0.1428,  0.0118, -0.1396, -0.1109],\n",
            "          [-0.0498, -0.0102, -0.1513,  0.0632, -0.1586],\n",
            "          [ 0.1208, -0.1810, -0.0081, -0.1967, -0.1024],\n",
            "          [ 0.1926, -0.1098,  0.1869,  0.1169,  0.1518],\n",
            "          [-0.1553,  0.0429, -0.0674, -0.1835,  0.0529]]]], requires_grad=True)\n",
            "layer_name <bound method Module.__repr__ of ReLU()>\n",
            "layer_name <bound method Module.__repr__ of Conv2d(2, 4, kernel_size=(7, 7), stride=(1, 1))>\n",
            "4\n",
            "torch.Size([4, 2, 7, 7])\n",
            "Parameter containing:\n",
            "tensor([[[[-7.7785e-02,  6.7378e-02, -3.0790e-02, -1.6949e-02, -3.6163e-02,\n",
            "            6.4802e-02, -4.2662e-02],\n",
            "          [-7.5502e-02, -2.2015e-02,  3.3356e-02,  6.2071e-02,  5.9743e-02,\n",
            "            8.5583e-02,  2.7202e-02],\n",
            "          [ 6.8190e-02, -1.1552e-02, -7.4982e-02, -1.9257e-02,  8.4108e-02,\n",
            "            6.6820e-02, -4.8481e-02],\n",
            "          [-4.0818e-02, -2.6565e-02,  3.4418e-02, -3.5851e-02, -9.4079e-02,\n",
            "            6.0035e-02, -1.1657e-02],\n",
            "          [ 9.3960e-02, -1.9111e-02, -6.1612e-03,  4.3200e-02,  4.4884e-02,\n",
            "           -6.7553e-03, -6.2284e-05],\n",
            "          [-4.3570e-02, -4.8475e-02, -6.0513e-02,  6.7853e-02, -5.0847e-02,\n",
            "           -7.4831e-02,  9.8875e-02],\n",
            "          [-9.2460e-02, -8.7493e-02, -7.8877e-02, -3.8645e-02,  3.1418e-02,\n",
            "           -8.2184e-02,  8.5331e-02]],\n",
            "\n",
            "         [[-8.5173e-02,  4.1725e-02, -4.8064e-02, -1.6174e-02, -6.1290e-02,\n",
            "            3.5221e-02,  6.2646e-02],\n",
            "          [-6.8399e-02,  2.3857e-02,  4.5791e-02, -5.2738e-02, -9.0224e-02,\n",
            "            2.1665e-02,  6.2714e-02],\n",
            "          [-3.1869e-02, -9.1688e-02, -1.3170e-02, -4.6571e-02, -3.7886e-03,\n",
            "            7.0003e-02, -4.5003e-02],\n",
            "          [ 3.3886e-02,  5.9673e-02, -1.5458e-02, -4.1908e-02,  4.9733e-04,\n",
            "            6.1031e-02, -2.3867e-02],\n",
            "          [ 1.8734e-03, -6.3117e-02,  4.4421e-02, -1.5909e-03, -9.0530e-02,\n",
            "           -3.7689e-02,  4.1286e-02],\n",
            "          [-6.5594e-02,  6.4286e-02,  5.5188e-02,  7.2954e-02,  7.8377e-03,\n",
            "           -2.3102e-02, -7.8147e-02],\n",
            "          [-5.5038e-02,  3.1168e-02,  7.1950e-02,  4.1432e-02, -4.4754e-02,\n",
            "            4.3242e-02, -4.7804e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.2796e-02, -5.0797e-02,  1.7932e-03,  5.3907e-02,  7.7633e-02,\n",
            "            9.7413e-03, -6.5656e-02],\n",
            "          [ 7.6154e-02, -7.8285e-02, -4.4695e-02, -3.5094e-02, -5.8861e-02,\n",
            "           -7.6346e-03,  8.5729e-03],\n",
            "          [-9.5198e-02,  4.6769e-02,  5.3275e-02, -2.9786e-02, -9.7047e-02,\n",
            "           -5.0084e-02,  3.7104e-02],\n",
            "          [ 3.0629e-02,  3.5180e-02,  5.7105e-03, -9.0216e-02, -8.9923e-02,\n",
            "           -9.2714e-02,  7.8141e-02],\n",
            "          [ 4.4982e-02,  4.1207e-02, -2.7596e-02, -6.5950e-03, -6.1433e-02,\n",
            "           -9.3170e-02,  3.0028e-02],\n",
            "          [-4.7384e-02,  7.8887e-02, -5.1761e-02, -9.1141e-02, -3.9283e-05,\n",
            "            7.5079e-02, -9.8253e-02],\n",
            "          [-8.9283e-02,  1.0065e-02, -4.7985e-02,  9.0944e-02,  1.3499e-02,\n",
            "            2.1645e-02,  2.2245e-02]],\n",
            "\n",
            "         [[-8.4062e-03, -3.2429e-02,  4.0833e-02,  4.9357e-02,  6.0505e-02,\n",
            "           -9.6315e-03,  2.4609e-02],\n",
            "          [-4.1361e-02, -4.2683e-02,  9.9312e-02, -8.9165e-02,  1.3222e-02,\n",
            "            4.4766e-02,  4.1638e-02],\n",
            "          [ 7.4965e-03, -2.1269e-04, -7.5251e-02,  6.5390e-02,  7.3599e-02,\n",
            "            8.2419e-02,  2.5523e-02],\n",
            "          [ 1.9679e-02,  6.1634e-02, -6.2934e-02, -8.8355e-02,  2.1110e-02,\n",
            "            1.0734e-02,  1.1229e-03],\n",
            "          [ 2.1692e-02,  4.5233e-02, -9.1290e-02, -3.7189e-02, -4.2145e-02,\n",
            "           -3.8568e-02,  9.9417e-02],\n",
            "          [-7.9706e-02,  3.0813e-03, -3.2471e-02,  2.8758e-02,  2.3237e-02,\n",
            "           -6.4968e-02, -2.6226e-02],\n",
            "          [-2.4435e-02,  9.6924e-02,  7.8865e-02, -3.3749e-02,  2.5685e-03,\n",
            "           -3.9113e-02, -3.2090e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.0468e-02,  1.3514e-02, -7.9246e-02,  1.0172e-01, -9.3315e-03,\n",
            "           -9.0518e-02,  2.5620e-02],\n",
            "          [-2.2211e-02,  1.3234e-03,  9.4342e-02,  1.9639e-02,  4.4861e-02,\n",
            "           -2.2801e-02,  2.9145e-02],\n",
            "          [ 3.8026e-02,  9.3108e-02, -1.3239e-02,  7.9233e-02,  5.6475e-02,\n",
            "           -4.9914e-02,  8.8596e-02],\n",
            "          [-2.2353e-02,  8.3906e-02, -1.8211e-02, -2.7467e-02,  2.2942e-02,\n",
            "            4.2235e-02, -6.3038e-02],\n",
            "          [-2.6239e-02, -1.3189e-02, -5.8327e-02,  2.2584e-02,  4.6337e-02,\n",
            "           -5.7043e-02, -2.8614e-02],\n",
            "          [ 5.4103e-02, -4.3530e-03, -6.1757e-03,  6.3884e-02, -8.9757e-02,\n",
            "            7.6822e-02,  1.0299e-01],\n",
            "          [-4.3117e-02, -2.4770e-02, -4.8205e-02, -2.8644e-02,  5.4689e-02,\n",
            "           -3.8398e-02,  6.9240e-02]],\n",
            "\n",
            "         [[-9.6475e-02,  2.5526e-02,  9.0988e-02,  3.1549e-02,  1.7516e-02,\n",
            "           -7.3025e-03, -8.2616e-02],\n",
            "          [ 8.3307e-02,  4.0358e-02, -5.8372e-03, -4.1926e-02, -7.1915e-02,\n",
            "            9.3573e-03,  4.8945e-02],\n",
            "          [ 1.5477e-02, -1.8427e-02, -7.5111e-02,  4.8547e-02, -3.2592e-02,\n",
            "           -3.5660e-02,  9.4680e-03],\n",
            "          [-7.3679e-02,  8.2107e-02, -2.6067e-02, -2.0836e-02,  9.1033e-02,\n",
            "           -2.1069e-02,  7.8876e-02],\n",
            "          [ 9.6910e-02,  2.3765e-02,  1.0865e-02, -7.4011e-02,  4.6144e-02,\n",
            "            9.4204e-02, -8.4741e-02],\n",
            "          [-2.0186e-02,  2.0063e-03,  5.5806e-02, -6.1601e-02,  1.0008e-01,\n",
            "           -1.2730e-02, -5.6581e-03],\n",
            "          [ 4.4233e-02,  3.5056e-02, -1.3864e-02,  2.2769e-02, -2.5750e-02,\n",
            "            9.7046e-02, -4.6366e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1809e-02,  7.7375e-02,  9.9536e-02, -3.9966e-02,  9.8383e-03,\n",
            "            4.5205e-02, -6.2002e-02],\n",
            "          [-9.5738e-02,  2.5895e-02,  6.7787e-02, -4.2990e-02,  4.3659e-02,\n",
            "           -8.7486e-02, -5.9087e-02],\n",
            "          [-3.6811e-02,  4.2709e-03, -4.6660e-03, -3.9632e-02,  7.6463e-02,\n",
            "           -1.1825e-02,  5.7865e-02],\n",
            "          [-6.3671e-02, -9.4688e-02,  3.7413e-02,  6.1229e-02,  6.2339e-02,\n",
            "           -6.6633e-02, -4.5555e-02],\n",
            "          [ 6.5308e-02,  7.1733e-02,  8.8475e-02,  5.2693e-02,  3.0189e-02,\n",
            "           -4.4628e-02,  8.4163e-02],\n",
            "          [ 2.8254e-02, -6.4504e-02, -8.3153e-03, -1.6533e-02, -8.9298e-02,\n",
            "           -6.8321e-02,  8.7149e-02],\n",
            "          [-1.1660e-02, -6.7966e-02, -4.3793e-02,  5.0402e-03,  3.5757e-02,\n",
            "           -9.6993e-02, -4.6569e-02]],\n",
            "\n",
            "         [[ 5.4557e-03, -9.7929e-02, -4.9150e-02, -1.0238e-01, -4.1505e-02,\n",
            "            5.0278e-02, -6.9870e-02],\n",
            "          [ 5.6990e-02, -2.6836e-02,  3.7629e-02,  6.7346e-02, -8.6733e-02,\n",
            "            2.4837e-02, -3.3023e-02],\n",
            "          [ 5.7259e-03, -3.0682e-02,  4.2492e-02, -6.5012e-02, -6.0639e-02,\n",
            "           -6.3232e-02, -1.6724e-02],\n",
            "          [-2.8529e-02,  4.3990e-02,  8.8823e-02, -2.2399e-02,  2.6903e-02,\n",
            "            1.5005e-02, -9.4577e-02],\n",
            "          [-3.8814e-02, -1.5202e-03, -6.7418e-02,  3.8804e-02, -9.1567e-03,\n",
            "           -9.5539e-02, -5.6626e-02],\n",
            "          [-7.3475e-02, -9.4709e-02, -7.5542e-02,  7.5337e-02,  4.0877e-02,\n",
            "           -3.6298e-02,  7.4450e-02],\n",
            "          [ 6.5116e-02, -5.0170e-02,  5.9696e-02, -8.7239e-02, -8.7401e-02,\n",
            "            1.0817e-03, -5.0443e-02]]]], requires_grad=True)\n",
            "layer_name <bound method Module.__repr__ of ReLU()>\n",
            "layer_name <bound method Module.__repr__ of Flatten(start_dim=1, end_dim=-1)>\n",
            "layer_name <bound method Module.__repr__ of Linear(in_features=1296, out_features=512, bias=True)>\n",
            "512\n",
            "torch.Size([512, 1296])\n",
            "Parameter containing:\n",
            "tensor([[-0.0179, -0.0230, -0.0214,  ...,  0.0028, -0.0135, -0.0139],\n",
            "        [ 0.0126,  0.0241, -0.0277,  ...,  0.0063,  0.0132,  0.0038],\n",
            "        [ 0.0153, -0.0127, -0.0116,  ..., -0.0098,  0.0098,  0.0208],\n",
            "        ...,\n",
            "        [ 0.0103, -0.0116,  0.0009,  ..., -0.0201,  0.0203, -0.0179],\n",
            "        [-0.0030,  0.0072, -0.0270,  ..., -0.0205, -0.0093, -0.0241],\n",
            "        [-0.0230, -0.0171, -0.0194,  ..., -0.0033, -0.0134, -0.0028]],\n",
            "       requires_grad=True)\n",
            "layer_name <bound method Module.__repr__ of ReLU()>\n",
            "layer_name <bound method Module.__repr__ of Linear(in_features=512, out_features=128, bias=True)>\n",
            "128\n",
            "torch.Size([128, 512])\n",
            "Parameter containing:\n",
            "tensor([[-0.0127,  0.0344,  0.0187,  ...,  0.0221,  0.0111,  0.0022],\n",
            "        [-0.0403,  0.0360, -0.0310,  ...,  0.0399, -0.0193, -0.0115],\n",
            "        [ 0.0088, -0.0243,  0.0087,  ..., -0.0324, -0.0433,  0.0255],\n",
            "        ...,\n",
            "        [-0.0204, -0.0017,  0.0311,  ..., -0.0170, -0.0164,  0.0432],\n",
            "        [ 0.0064, -0.0450, -0.0247,  ..., -0.0109, -0.0042, -0.0236],\n",
            "        [ 0.0461,  0.0372,  0.0113,  ...,  0.0391, -0.0175, -0.0097]],\n",
            "       requires_grad=True)\n",
            "layer_name <bound method Module.__repr__ of ReLU()>\n",
            "layer_name <bound method Module.__repr__ of Linear(in_features=128, out_features=32, bias=True)>\n",
            "32\n",
            "torch.Size([32, 128])\n",
            "Parameter containing:\n",
            "tensor([[-0.0259, -0.0717,  0.0747,  ..., -0.0747, -0.0532, -0.0644],\n",
            "        [-0.0628,  0.0509,  0.0516,  ...,  0.0874,  0.0412,  0.0600],\n",
            "        [-0.0673, -0.0676, -0.0497,  ..., -0.0502, -0.0339, -0.0446],\n",
            "        ...,\n",
            "        [ 0.0682,  0.0447, -0.0237,  ..., -0.0472,  0.0425,  0.0403],\n",
            "        [ 0.0028, -0.0757, -0.0795,  ...,  0.0147, -0.0153, -0.0183],\n",
            "        [-0.0004, -0.0544, -0.0345,  ..., -0.0488, -0.0313, -0.0296]],\n",
            "       requires_grad=True)\n",
            "layer_name <bound method Module.__repr__ of ReLU()>\n",
            "layer_name <bound method Module.__repr__ of Linear(in_features=32, out_features=10, bias=True)>\n",
            "10\n",
            "torch.Size([10, 32])\n",
            "Parameter containing:\n",
            "tensor([[ 0.0402, -0.1039, -0.0745,  0.1690,  0.0199,  0.0622,  0.0276, -0.1664,\n",
            "          0.1118, -0.0520, -0.1423,  0.0435, -0.1097,  0.1424,  0.1288, -0.0679,\n",
            "         -0.0806, -0.0955,  0.0864,  0.1102, -0.0832, -0.0142, -0.0876, -0.1597,\n",
            "          0.0150, -0.0760,  0.1684,  0.0717,  0.1748,  0.0265, -0.1417, -0.0976],\n",
            "        [ 0.1398,  0.0589, -0.1677,  0.0591, -0.0363,  0.0454, -0.0409,  0.0706,\n",
            "         -0.1221,  0.0198,  0.1460, -0.0668, -0.0304,  0.0014, -0.0664,  0.0256,\n",
            "         -0.0775,  0.1373, -0.0256,  0.1609,  0.1570,  0.0398,  0.0191,  0.0096,\n",
            "          0.0352, -0.1330, -0.1494, -0.1665, -0.0586, -0.0943, -0.1135,  0.0159],\n",
            "        [-0.0434,  0.1349,  0.1146,  0.0592, -0.0649, -0.0877,  0.1384, -0.1755,\n",
            "         -0.1754,  0.0761,  0.0927,  0.0479, -0.0949, -0.0061,  0.1588,  0.0647,\n",
            "         -0.0338,  0.1726, -0.1119,  0.1584,  0.0502, -0.0079, -0.0721, -0.0167,\n",
            "         -0.0101, -0.0268,  0.0887,  0.1015, -0.0052, -0.0204,  0.0601, -0.1189],\n",
            "        [ 0.0127,  0.1446, -0.1315, -0.0514, -0.1623,  0.1472, -0.0114,  0.1060,\n",
            "          0.0802,  0.1572, -0.0416, -0.0990,  0.1778,  0.1527, -0.0057, -0.1534,\n",
            "          0.0931, -0.1479,  0.1101,  0.0109,  0.0202,  0.0696,  0.0441,  0.1295,\n",
            "          0.0980, -0.0276,  0.1649, -0.1403,  0.1760,  0.0548,  0.0580,  0.1130],\n",
            "        [ 0.0520,  0.1007,  0.1575, -0.1514,  0.1349, -0.0291,  0.1283, -0.1665,\n",
            "         -0.0256, -0.1449,  0.1102, -0.0354,  0.0428, -0.0776, -0.0609,  0.1446,\n",
            "         -0.0460,  0.0673,  0.0864, -0.0971, -0.1076,  0.1496, -0.0386,  0.1266,\n",
            "          0.0341,  0.1036, -0.1360, -0.0095, -0.0401,  0.1433, -0.0471,  0.0974],\n",
            "        [-0.1631, -0.0873,  0.1350,  0.0728,  0.0212, -0.0297, -0.0665,  0.1422,\n",
            "         -0.1531,  0.1409,  0.1371, -0.1033,  0.1600,  0.1606, -0.0022, -0.0447,\n",
            "          0.0377, -0.0023,  0.0909, -0.0530, -0.1510,  0.0077,  0.1650,  0.1198,\n",
            "         -0.0005, -0.1719,  0.1340,  0.0921,  0.1485,  0.0986,  0.0499,  0.0751],\n",
            "        [-0.0593,  0.0105,  0.0981,  0.1207, -0.1546,  0.0783, -0.0581,  0.0345,\n",
            "         -0.1270, -0.0984,  0.0796,  0.0585, -0.1351, -0.1039, -0.1604,  0.0206,\n",
            "         -0.1268, -0.1226,  0.0127, -0.1036,  0.1346,  0.1563,  0.0005,  0.0460,\n",
            "          0.0723,  0.0744,  0.0388,  0.1688,  0.0794,  0.1452,  0.1438,  0.1290],\n",
            "        [-0.1698,  0.1087,  0.1168,  0.0657,  0.1778, -0.1167,  0.0123,  0.0504,\n",
            "          0.1660, -0.0165, -0.1450, -0.1452,  0.1195,  0.1564,  0.0761, -0.1572,\n",
            "         -0.0500,  0.1656, -0.0169, -0.1312,  0.1638, -0.0600,  0.1707,  0.0510,\n",
            "          0.0419, -0.0472, -0.1684, -0.1210, -0.0698,  0.1464,  0.1699, -0.1124],\n",
            "        [-0.1519,  0.1451, -0.0763,  0.0209, -0.0140, -0.0684,  0.0692, -0.0478,\n",
            "         -0.0594, -0.1604,  0.1145, -0.1052, -0.0701,  0.0649,  0.0940, -0.1696,\n",
            "          0.1532, -0.0749, -0.1412, -0.0580,  0.1711, -0.1106,  0.0759,  0.0865,\n",
            "         -0.0896, -0.1511,  0.0370,  0.1741,  0.1476, -0.1369,  0.1333, -0.0454],\n",
            "        [ 0.1021,  0.0688,  0.1715, -0.0290,  0.0500,  0.1451, -0.0291, -0.0812,\n",
            "         -0.0951,  0.0542,  0.0436,  0.0334,  0.1408, -0.1508,  0.0805, -0.1008,\n",
            "         -0.0672, -0.1046,  0.1451, -0.1488, -0.1666, -0.1715, -0.1379,  0.0163,\n",
            "          0.1295, -0.0760,  0.0429,  0.0458, -0.0093, -0.1119, -0.0536,  0.1140]],\n",
            "       requires_grad=True)\n",
            "layer_name <bound method Module.__repr__ of Softmax(dim=1)>\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uint16-custom2\n",
        "\n",
        "\n",
        "{'Epoch': 1, 'Loss': 2.30095}: 100%|██████████| 164/164 [00:59<00:00,  2.74it/s]\n",
        "Test Accuracy: 0.09963\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 2, 'Loss': 2.09561}: 100%|██████████| 164/164 [00:57<00:00,  2.84it/s]\n",
        "Test Accuracy: 0.36056\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 3, 'Loss': 1.86639}: 100%|██████████| 164/164 [00:58<00:00,  2.81it/s]\n",
        "Test Accuracy: 0.63837\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 4, 'Loss': 1.77721}: 100%|██████████| 164/164 [00:56<00:00,  2.90it/s]\n",
        "Test Accuracy: 0.68152\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 5, 'Loss': 1.74531}: 100%|██████████| 164/164 [01:01<00:00,  2.68it/s]\n",
        "Test Accuracy: 0.71413\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 6, 'Loss': 1.72985}: 100%|██████████| 164/164 [00:44<00:00,  3.71it/s]\n",
        "Test Accuracy: 0.73011\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 7, 'Loss': 1.64313}: 100%|██████████| 164/164 [01:02<00:00,  2.63it/s]\n",
        "Test Accuracy: 0.73162\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 8, 'Loss': 1.54832}: 100%|██████████| 164/164 [00:42<00:00,  3.83it/s]\n",
        "Test Accuracy: 0.90883\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 9, 'Loss': 1.53205}: 100%|██████████| 164/164 [00:22<00:00,  7.37it/s]\n",
        "Test Accuracy: 0.92051\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 10, 'Loss': 1.52779}: 100%|██████████| 164/164 [00:40<00:00,  4.01it/s]\n",
        "Test Accuracy: 0.92549\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# uint8-custom 1\n",
        "{'Epoch': 1, 'Loss': 2.30106}: 100%|██████████| 164/164 [00:54<00:00,  3.00it/s]\n",
        "Test Accuracy: 0.10106\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 2, 'Loss': 2.13939}: 100%|██████████| 164/164 [00:44<00:00,  3.65it/s]\n",
        "Test Accuracy: 0.38529\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 3, 'Loss': 1.98048}: 100%|██████████| 164/164 [00:23<00:00,  6.95it/s]\n",
        "Test Accuracy: 0.44234\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 4, 'Loss': 1.82871}: 100%|██████████| 164/164 [00:43<00:00,  3.76it/s]\n",
        "Test Accuracy: 0.64131\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 5, 'Loss': 1.77463}: 100%|██████████| 164/164 [00:25<00:00,  6.49it/s]\n",
        "Test Accuracy: 0.68517\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 6, 'Loss': 1.75027}: 100%|██████████| 164/164 [00:24<00:00,  6.64it/s]\n",
        "Test Accuracy: 0.70226\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 7, 'Loss': 1.73521}: 100%|██████████| 164/164 [00:47<00:00,  3.49it/s]\n",
        "Test Accuracy: 0.71979\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 8, 'Loss': 1.71573}: 100%|██████████| 164/164 [00:44<00:00,  3.67it/s]\n",
        "Test Accuracy: 0.73043\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 9, 'Loss': 1.68576}: 100%|██████████| 164/164 [00:53<00:00,  3.06it/s]\n",
        "Test Accuracy: 0.73663\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "{'Epoch': 10, 'Loss': 1.64119}: 100%|██████████| 164/164 [00:21<00:00,  7.65it/s]\n",
        "Test Accuracy: 0.8215\n",
        "Compression Ratio  99.97330216770052\n",
        "start_size: 2867.7421875,end_size: 784\n",
        "Time Taken:  399.71174716949463"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
