{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Apr 28 07:08:16 2024\n",
        "\n",
        "\n",
        "@author: Faiza\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import struct\n",
        "from typing import Tuple\n",
        "from bitarray import bitarray\n",
        "# from src.compressors.compressor import Compressor\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from load_dataset import Dataset\n",
        "import os\n",
        "\n",
        "import time\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "import cv2\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "import math\n",
        "# from src.compressors.compressor import Compressor\n",
        "import struct\n",
        "\n",
        "\n",
        "from bitarray import bitarray\n",
        "from bitarray.util import int2ba, ba2int\n",
        "np.random.seed(786)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Compressor(ABC):\n",
        "    @abstractmethod\n",
        "    def encode(self, array: np.ndarray) -> bytes:\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def decode(self, array: bytes) -> np.ndarray:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Existing Quantization schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\FedPAQ_env\\lib\\site-packages\\julia\\juliainfo.py:93: UserWarning: julia warned:\n",
            "The latest version of Julia in the `release` channel is 1.10.5+0.x64.w64.mingw32. You currently have `1.10.2+0.x64.w64.mingw32` installed. Run:\n",
            "\n",
            "  juliaup update\n",
            "\n",
            "in your terminal shell to install Julia 1.10.5+0.x64.w64.mingw32 and update the `release` channel to that version.\n",
            "  warnings.warn(\"{} warned:\\n{}\".format(julia, stderr))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QSGD\n",
            "Before Quantization:  [7.5668163  3.074451   8.939189   3.866898   8.738546   0.76235867\n",
            " 3.765722   3.2655404  9.408178   4.831219   5.5546727  3.34706\n",
            " 2.8331633  5.110989   5.5942545  6.458654  ]\n",
            "Encoded b'(\\x00\\x00\\x00\\x114\\xb7Ar)E&\\x05\\x00\\x00\\x00'\n",
            "After Quantization:  [11.450212  0.        0.        0.       11.450212  0.       11.450212\n",
            "  0.       11.450212 11.450212  0.        0.       11.450212  0.\n",
            " 11.450212  0.      ]\n",
            "52.88461538461539% smaller \n",
            "70.83333333333333% smaller \n"
          ]
        }
      ],
      "source": [
        "from julia.api import Julia\n",
        "jl = Julia(compiled_modules=False)\n",
        "from julia import Main\n",
        "Main.eval(\"using Random; Random.seed!(0)\")\n",
        "Main.include(\"src/compressors/qsgd.jl\")\n",
        "\n",
        "\n",
        "class QSGD(Compressor):\n",
        "    def __init__(self, s: int, zero_rle: bool, type=None):\n",
        "        np.random.seed(16841351)\n",
        "        self.type = type\n",
        "        if self.type == \"LFL\":\n",
        "            self.s = 2\n",
        "        else:\n",
        "            self.s = s\n",
        "        self.zero_rle = zero_rle\n",
        "\n",
        "    # format:    | length | norm | s(1) | sign(1) | s(2) | ... | sign(n) | \n",
        "    # (no 0-rle) | 32     | 32   | ?    | 1       | ?    | ... | 1       |\n",
        "    # format:    | length | norm | n_zeros | s(1) | sign(1) | n_zeros | s(2) | ... | sign(n) |\n",
        "    # (0-rle)    | 32     | 32   | ?       | ?    | 1       | ?       | ?    | ... | 1       |\n",
        "    def encode(self, array: np.ndarray, seed=None, cid=\":-)\", client_name=\":-))\") -> bytes:\n",
        "        assert len(array.shape) == 1\n",
        "        assert array.dtype == np.float32\n",
        "        result = Main.encode_qsgd(array, self.s, self.type, seed, cid, client_name)\n",
        "        return bytes(result)\n",
        "\n",
        "    def decode(self, array: bytes, use_lo_quant=False) -> np.ndarray:\n",
        "        result = Main.decode_qsgd(array, self.s, self.type, use_lo_quant)\n",
        "        return result\n",
        "    \n",
        "#%% testing qsgd quantization\n",
        "\n",
        "print(\"QSGD\")\n",
        "arr = list(np.random.rand(1,16).flatten()*10)\n",
        "arr = np.array(arr, dtype = np.float32)\n",
        "print(\"Before Quantization: \", arr)\n",
        "cmp = QSGD(2,True)\n",
        "enc = cmp.encode(arr)\n",
        "print(\"Encoded\",enc)\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{100 * (sys.getsizeof(arr)-sys.getsizeof(enc))/sys.getsizeof(arr)}% smaller \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GZip\n",
            "Before Quantization:  [7.819443  5.889166  6.3414087 5.9300547 6.9216805 6.8079643 6.2213473\n",
            " 8.728382  7.2029533 9.9790745 9.917385  0.910352  6.12871   4.920648\n",
            " 4.4214873 6.2949104]\n",
            "Encoded b'\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\ncd\\x00\\x01F0\\x02\\x81\\xe5\\xf1\\xb7\\x1d\\x01\\x9e\\xbbx\\xe2\\x14\\x00\\x00\\x00'\n",
            "After Quantization:  [27.421705  0.        0.        0.        0.        0.       27.421705\n",
            "  0.        0.       27.421705  0.        0.        0.        0.\n",
            "  0.        0.      ]\n",
            "38.46153846153846% smaller \n",
            "61.904761904761905% smaller \n"
          ]
        }
      ],
      "source": [
        "#%%SETTING UP Julia and gzip Class\n",
        "from julia.api import Julia\n",
        "jl = Julia(compiled_modules=False)\n",
        "from julia import Main\n",
        "Main.eval(\"using Random; Random.seed!(0)\")\n",
        "Main.include(\"src/compressors/gzip.jl\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GZip(Compressor):\n",
        "    def __init__(self, s: int):\n",
        "        np.random.seed(786)\n",
        "        self.s = s\n",
        "\n",
        "    def encode(self, array: np.ndarray, seed=None, cid=\":-)\", client_name=\":-))\") -> bytes:\n",
        "        assert len(array.shape) == 1\n",
        "        assert array.dtype == np.float32\n",
        "        result = Main.encode_gzip(array, self.s, seed)\n",
        "        return bytes(result)\n",
        "\n",
        "    def decode(self, array: bytes, use_lo_quant=False) -> np.ndarray:\n",
        "        result = Main.decode_gzip(array, self.s)\n",
        "        return result\n",
        "\n",
        "\n",
        "#%% testing gzip quantization\n",
        "print(\"GZip\")\n",
        "arr = list(np.random.rand(1,16).flatten()*10)\n",
        "arr = np.array(arr, dtype = np.float32)\n",
        "print(\"Before Quantization: \", arr)\n",
        "cmp = GZip(1)\n",
        "enc = cmp.encode(arr)\n",
        "print(\"Encoded\",enc)\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{100 * (sys.getsizeof(arr)-sys.getsizeof(enc))/sys.getsizeof(arr)}% smaller \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP8\n",
            "Before Quantization:  [7.5668163  3.074451   8.939189   3.866898   8.738546   0.76235867\n",
            " 3.765722   3.2655404  9.408178   4.831219   5.5546727  3.34706\n",
            " 2.8331633  5.110989   5.5942545  6.458654  ]\n",
            "Encoded b'HBHDH:DBIEEBBEFF'\n",
            "After Quantization:  [ 8.    3.    8.    4.    8.    0.75  4.    3.   10.    5.    5.    3.\n",
            "  3.    5.    6.    6.  ]\n",
            "52.88461538461539% smaller \n",
            "70.83333333333333% smaller \n"
          ]
        }
      ],
      "source": [
        "#%%FP8\n",
        "\n",
        "# print(\"Starting up Julia.\")\n",
        "from julia.api import Julia\n",
        "jl = Julia(compiled_modules=False)\n",
        "from julia import Main\n",
        "Main.eval(\"using Random; Random.seed!(0)\")\n",
        "Main.include(\"src/compressors/fp8.jl\")\n",
        "# print(\"Finished starting up Julia.\")\n",
        "\n",
        "FP8_FORMAT = (1, 5, 2)\n",
        "FP32_FORMAT = (1, 8, 23)\n",
        "\n",
        "def get_emax(format):\n",
        "    return (2**(format[1]-1)) - 1\n",
        "\n",
        "def get_emin(format):\n",
        "    return 1 - get_emax(format)\n",
        "\n",
        "\n",
        "class FP8(Compressor):\n",
        "    def __init__(self):\n",
        "        np.random.seed(786)\n",
        "        self.fp8s_repr_in_fp32 = []\n",
        "        self.fp8s = []\n",
        "        self.s = -1\n",
        "        # negative values before positive values.\n",
        "        def insert(num):\n",
        "            byte = struct.pack('>B', num)\n",
        "            [num] = self.decode(byte)\n",
        "            if not np.isnan(num):\n",
        "                self.fp8s.append(byte)\n",
        "                self.fp8s_repr_in_fp32.append(num)\n",
        "                bits = bitarray()\n",
        "                bits.frombytes(byte)\n",
        "        for i in list(reversed(range(128, 253))) + list(range(0, 128)):\n",
        "            insert(i)\n",
        "        self.fp8s_repr_in_fp32 = np.array(self.fp8s_repr_in_fp32).astype(np.float32)\n",
        "\n",
        "    def get_fp8_neighbors(self, f: np.float32) -> Tuple[bytes, bytes]:\n",
        "        idx_high = np.searchsorted(self.fp8s_repr_in_fp32, f, side='right')\n",
        "        idx_low = idx_high - 1\n",
        "        if idx_high == len(self.fp8s_repr_in_fp32):\n",
        "            idx_high -= 1\n",
        "        return self.fp8s[idx_low], self.fp8s[idx_high], self.fp8s_repr_in_fp32[idx_low], self.fp8s_repr_in_fp32[idx_high]\n",
        "\n",
        "    def encode(self, array: np.ndarray, seed=None, cid=\":-)\", client_name=\":-))\") -> bytes:\n",
        "        assert len(array.shape) == 1\n",
        "        assert array.dtype == np.float32\n",
        "        result = Main.encode_fp8(array, self.fp8s_repr_in_fp32, self.fp8s, seed)\n",
        "        return bytes(result)\n",
        "    \n",
        "    def decode(self, array: bytes, use_lo_quant=False) -> np.ndarray:\n",
        "        result = Main.decode_fp8(array)\n",
        "        return result\n",
        "#%% testing fp32 quantization\n",
        "print(\"FP8\")\n",
        "arr = list(np.random.rand(1,16).flatten()*10)\n",
        "arr = np.array(arr, dtype = np.float32)\n",
        "print(\"Before Quantization: \", arr)\n",
        "cmp = FP8()\n",
        "enc = cmp.encode(arr)\n",
        "print(\"Encoded\",enc)\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{100 * (sys.getsizeof(arr)-sys.getsizeof(enc))/sys.getsizeof(arr)}% smaller \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7.5668163  3.074451   8.939189   3.866898   8.738546   0.76235867\n",
            " 3.765722   3.2655404  9.408178   4.831219   5.5546727  3.34706\n",
            " 2.8331633  5.110989   5.5942545  6.458654  ]\n",
            "Encoded [200  68 241  91 235   0  88  73 255 120 141  76  61 128 142 168]\n",
            "After Quantization:  [7.5433937  3.06791058 8.93350588 3.8477296  8.73007483 0.76235867\n",
            " 3.74601408 3.23743645 9.40817833 4.83097968 5.54298836 3.33915198\n",
            " 2.83057435 5.10222109 5.57689354 6.45842809]\n",
            "<class 'numpy.uint8'> 48.275862068965516% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  0.00017657975556106007\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "class Custom_linear():\n",
        "    def __init__(self, quantization_type = np.uint32):\n",
        "        np.random.seed(348543)\n",
        "        self.quantization_type = quantization_type\n",
        "        #quantization_level from 0 to 100\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self,data):\n",
        "        # data = data.astype(int)\n",
        "        self.xp = [data.min(), data.max()]\n",
        "        min = np.iinfo(self.quantization_type).min\n",
        "        max = np.iinfo(self.quantization_type).max\n",
        "\n",
        "        self.fp = [min, max]\n",
        "        enc = np.interp(data, self.xp, self.fp)\n",
        "        enc = enc.astype(self.quantization_type)\n",
        "        # encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quantization_level] \n",
        "        # result, encimg = cv2.imencode('.jpg', img, encode_param)\n",
        "\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        dec = np.interp(enc, self.fp, self.xp)\n",
        "        return dec\n",
        "    \n",
        "class Custom_linear_16():\n",
        "    def __init__(self, quantization_type = np.uint16):\n",
        "        np.random.seed(786)\n",
        "        self.quantization_type = quantization_type\n",
        "        #quantization_level from 0 to 100\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self,data):\n",
        "        # data = data.astype(int)\n",
        "        self.xp = [data.min(), data.max()]\n",
        "        min = np.iinfo(self.quantization_type).min\n",
        "        max = np.iinfo(self.quantization_type).max\n",
        "\n",
        "        self.fp = [min, max]\n",
        "        enc = np.interp(data, self.xp, self.fp)\n",
        "        enc = enc.astype(self.quantization_type)\n",
        "        # encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quantization_level] \n",
        "        # result, encimg = cv2.imencode('.jpg', img, encode_param)\n",
        "\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        dec = np.interp(enc, self.fp, self.xp)\n",
        "        return dec\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# data = np.random.rand(1,16)\n",
        "\n",
        "data = list(np.random.rand(1,16).flatten()*10)\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint8\n",
        "cmp = Custom_linear_16(data_type)\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "# print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "168\n",
            "232\n",
            "120\n"
          ]
        }
      ],
      "source": [
        "print(sys.getsizeof(data))\n",
        "print(sys.getsizeof(dec))\n",
        "print(sys.getsizeof(enc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Frequency Domain Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1099769  0.67525536 0.63571453 0.59198946 0.35005942 0.45300266\n",
            " 0.17287356 0.86327183 0.60224235 0.7167216  0.8245873  0.8214089\n",
            " 0.15033731 0.52826804 0.6807281  0.8829001 ]\n",
            "Encoded [ 0.0745  0.7983  0.851   0.9     0.508   0.6826  0.3687  0.9517  0.786\n",
            "  0.6484  0.6855  0.4744 -0.1088  0.3604  0.4314  0.3118]\n",
            "After Quantization:  [0.11004972 0.6753716  0.6357201  0.59214085 0.3499422  0.45291573\n",
            " 0.17291637 0.863149   0.6020095  0.716897   0.8244399  0.82149005\n",
            " 0.15035003 0.5281627  0.6806638  0.8829659 ]\n",
            "<class 'numpy.uint16'> 19.047619047619047% smaller \n",
            "<class 'numpy.uint16'> 19.047619047619047% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  1.3312858e-08\n"
          ]
        }
      ],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class customFreq():\n",
        "    def __init__(self, precision_levels = [16, 8, 4]):\n",
        "    # def __init__(self, precision_levels = [32, 16, 8]):\n",
        "        np.random.seed(786)\n",
        "        self.precision_levels = precision_levels\n",
        "        \n",
        "        # quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(dct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(idct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        \n",
        "        # Calculate importance as the magnitude of the frequency components\n",
        "        importance = np.abs(weights_f)\n",
        "        \n",
        "        # Assign precision based on importance\n",
        "        mean_importance = np.mean(importance)\n",
        "        if mean_importance > 0.1:\n",
        "            precision = self.precision_levels[0]  # 8-bit\n",
        "        elif mean_importance > 0.01:\n",
        "            precision = self.precision_levels[1]  # 4-bit\n",
        "        else:\n",
        "            precision = self.precision_levels[2]  # 2-bit\n",
        "        \n",
        "        # Quantize frequency components\n",
        "        enc = self.quantize(weights_f, precision)\n",
        "        enc = enc.astype(np.float16)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        weights_quantized = self.inverse_dct_transform(enc)\n",
        "        return weights_quantized\n",
        "    \n",
        "\n",
        "data = list(np.random.rand(1,16).flatten())\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customFreq()\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7566816  0.3074451  0.8939189  0.3866898  0.87385464 0.07623586\n",
            " 0.3765722  0.32655403 0.9408179  0.48312193 0.55546725 0.334706\n",
            " 0.28331634 0.5110989  0.5594255  0.6458654 ]\n",
            "Encoded [0.9644 0.3823 1.071  0.5923 0.9175 0.3452 0.346  0.4915 0.8613 0.4775\n",
            " 0.2651 0.2072 0.101  0.3909 0.2537 0.1909]\n",
            "After Quantization:  [0.75657487 0.30746344 0.89398825 0.38658762 0.8740002  0.07611474\n",
            " 0.3765901  0.32644805 0.9406408  0.4829445  0.5554271  0.3346513\n",
            " 0.28326702 0.5111246  0.5593281  0.64587265]\n",
            "<class 'numpy.uint16'> 19.047619047619047% smaller \n",
            "<class 'numpy.uint16'> 19.047619047619047% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  9.65502e-09\n"
          ]
        }
      ],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class customFreq_1_5():\n",
        "    # def __init__(self, precision_levels = [8, 4, 2]):\n",
        "    def __init__(self, precision_levels = [32, 16, 8]):\n",
        "        np.random.seed(786)\n",
        "        self.precision_levels = precision_levels\n",
        "        \n",
        "        #quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(dct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(idct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        \n",
        "        # Calculate importance as the magnitude of the frequency components\n",
        "        importance = np.abs(weights_f)\n",
        "        \n",
        "        # Assign precision based on importance\n",
        "        mean_importance = np.mean(importance)\n",
        "        if mean_importance > 0.1:\n",
        "            precision = self.precision_levels[0]  # 8-bit\n",
        "        elif mean_importance > 0.01:\n",
        "            precision = self.precision_levels[1]  # 4-bit\n",
        "        else:\n",
        "            precision = self.precision_levels[2]  # 2-bit\n",
        "        \n",
        "        # Quantize frequency components\n",
        "        enc = self.quantize(weights_f, precision)\n",
        "        enc = enc.astype(np.float16)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        weights_quantized = self.inverse_dct_transform(enc)\n",
        "        return weights_quantized\n",
        "    \n",
        "\n",
        "data = list(np.random.rand(1,16).flatten())\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customFreq_1_5()\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7566816  0.3074451  0.8939189  0.3866898  0.87385464 0.07623586\n",
            " 0.3765722  0.32655403 0.9408179  0.48312193 0.55546725 0.334706\n",
            " 0.28331634 0.5110989  0.5594255  0.6458654 ]\n",
            "Encoded [ 2.078    0.11017  0.1637   0.0697   0.2166  -0.4165  -0.1231   0.2578\n",
            "  0.1963   0.05832 -0.08093  0.07806  0.2084   0.072    0.4219   0.5474 ]\n",
            "After Quantization:  [0.7566943  0.3074998  0.8939637  0.3867146  0.8738843  0.07625042\n",
            " 0.37666655 0.32655337 0.94095886 0.4831015  0.55563855 0.33468464\n",
            " 0.28341928 0.51103944 0.5595117  0.6459183 ]\n",
            "<class 'numpy.uint16'> 19.047619047619047% smaller \n",
            "<class 'numpy.uint16'> 19.047619047619047% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  5.639836e-09\n"
          ]
        }
      ],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class customFreq_singleDCT():\n",
        "    # def __init__(self, precision_levels = [8, 4, 2]):\n",
        "    def __init__(self, precision_levels = [32, 16, 8]):\n",
        "        np.random.seed(786)\n",
        "        self.precision_levels = precision_levels\n",
        "        #quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(x, axis=-1, norm='ortho')\n",
        "\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(x, axis=-1, norm='ortho')\n",
        "\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        \n",
        "        # Calculate importance as the magnitude of the frequency components\n",
        "        importance = np.abs(weights_f)\n",
        "        \n",
        "        # Assign precision based on importance\n",
        "        mean_importance = np.mean(importance)\n",
        "        if mean_importance > 0.1:\n",
        "            precision = self.precision_levels[0]  # 8-bit\n",
        "        elif mean_importance > 0.01:\n",
        "            precision = self.precision_levels[1]  # 4-bit\n",
        "        else:\n",
        "            precision = self.precision_levels[2]  # 2-bit\n",
        "        \n",
        "        # Quantize frequency components\n",
        "        enc = self.quantize(weights_f, precision)\n",
        "        enc = enc.astype(np.float16)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        weights_quantized = self.inverse_dct_transform(enc)\n",
        "        return weights_quantized\n",
        "    \n",
        "\n",
        "data = list(np.random.rand(1,16).flatten())\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customFreq_singleDCT()\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Freq 3 (CustomFreq + Custom2 but more sophisticated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7566816  0.3074451  0.8939189  0.3866898  0.87385464 0.07623586\n",
            " 0.3765722  0.32655403 0.9408179  0.48312193 0.55546725 0.334706\n",
            " 0.28331634 0.5110989  0.5594255  0.6458654 ]\n",
            "32\n",
            "Encoded [3822393318 1245889824 4294967295 2175506503 3614236915 1081460852\n",
            " 1084719233 1728903523 3366925843 1667425989  726524250  469755745\n",
            "          0 1282924090  676045071  397722513]\n",
            "After Quantization:  [0.75668158 0.30744515 0.89391888 0.38668986 0.87385463 0.07623591\n",
            " 0.37657221 0.32655406 0.94081779 0.48312199 0.55546728 0.33470603\n",
            " 0.28331629 0.51109892 0.55942545 0.64586541]\n",
            "<class 'numpy.uint16'> 27.586206896551722% smaller \n",
            "<class 'numpy.uint16'> 0.0% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  1.7853008299711994e-15\n"
          ]
        }
      ],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class Linear_quant():\n",
        "    def __init__(self, quantization_type = np.uint32):\n",
        "        np.random.seed(786)\n",
        "        self.quantization_type = quantization_type\n",
        "        #quantization_level from 0 to 100\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self,data):\n",
        "        # data = data.astype(int)\n",
        "        self.xp = [data.min(), data.max()]\n",
        "        min = np.iinfo(self.quantization_type).min\n",
        "        max = np.iinfo(self.quantization_type).max\n",
        "\n",
        "        self.fp = [min, max]\n",
        "        enc = np.interp(data, self.xp, self.fp)\n",
        "        enc = enc.astype(self.quantization_type)\n",
        "        # encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quantization_level] \n",
        "        # result, encimg = cv2.imencode('.jpg', img, encode_param)\n",
        "        return enc\n",
        "    \n",
        "    def decode(self, enc) :\n",
        "        dec = np.interp(enc, self.fp, self.xp)\n",
        "        return dec\n",
        "    \n",
        "\n",
        "class customFreq_3():\n",
        "    # def __init__(self, precision_levels = [8, 4, 2]):\n",
        "    def __init__(self, precision_levels = [np.uint32, np.uint16, np.uint8]):\n",
        "        np.random.seed(786)\n",
        "        self.precision_levels = precision_levels\n",
        "        self.linearQuant_0 = Linear_quant(self.precision_levels[0])\n",
        "        self.linearQuant_1 = Linear_quant(self.precision_levels[1])\n",
        "        self.linearQuant_2 = Linear_quant(self.precision_levels[2])\n",
        "        \n",
        "        #quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(dct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(idct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        \n",
        "        # Calculate importance as the magnitude of the frequency components\n",
        "        importance = np.abs(weights_f)\n",
        "        \n",
        "        # Assign precision based on importance\n",
        "        self.mean_importance = np.mean(importance)\n",
        "        # if self.mean_importance > 0.1:\n",
        "        enc = self.linearQuant_0.encode(weights_f)  # 8-bit\n",
        "        precision = int(str(self.linearQuant_0.quantization_type).split('t')[-1].split('\\'')[0])\n",
        "        print(precision)\n",
        "\n",
        "        # elif self.mean_importance > 0.01:\n",
        "        #     enc = self.linearQuant_1.encode(weights_f)   # 4-bit\n",
        "        #     precision = int(str(self.linearQuant_1.quantization_type).split('t')[-1].split('\\'')[0])\n",
        "        #     # enc = self.quantize(weights_f, 16)\n",
        "        # else:\n",
        "        #     enc = self.linearQuant_2.encode(weights_f)   # 2-bit\n",
        "        #     precision = int(str(self.linearQuant_2.quantization_type).split('t')[-1].split('\\'')[0])\n",
        "\n",
        "        \n",
        "        # Quantize frequency components\n",
        "        # enc = self.quantize(enc, precision)\n",
        "        return enc\n",
        "\n",
        "\n",
        "    def decode(self, enc) :\n",
        "\n",
        "        # if self.mean_importance > 0.1:\n",
        "        weights_quantized = self.linearQuant_0.decode(enc)  # 8-bit\n",
        "        # elif self.mean_importance > 0.01:\n",
        "        #     weights_quantized = self.linearQuant_1.decode(enc)   # 4-bit\n",
        "        # else:\n",
        "        #     weights_quantized = self.linearQuant_2.decode(enc)   # 2-bit\n",
        "        weights_quantized = self.inverse_dct_transform(weights_quantized)\n",
        "        return weights_quantized\n",
        "    \n",
        "    \n",
        "data = list(np.random.rand(1,16).flatten())\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customFreq_3()\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CustomFreq2_singleDCT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.2028032  0.656495   0.20027462 0.01039412 0.38331908 0.49089465\n",
            " 0.8527781  0.3933121  0.08151923 0.23411173 0.17995436 0.38348877\n",
            " 0.10843034 0.8128075  0.1651963  0.8148211 ]\n",
            "Encoded [4294967295  613548904 1230186606    1948507 1446143072 1756172878\n",
            " 1351158514  308232491  246669602  527585713 1002354286          0\n",
            "  950578213   19140507 1865841127  594839003]\n",
            "After Quantization:  [0.20280322 0.65649494 0.20027461 0.01039412 0.3833191  0.49089464\n",
            " 0.85277808 0.39331213 0.08151923 0.23411172 0.1799544  0.38348874\n",
            " 0.10843033 0.81280747 0.16519627 0.81482112]\n",
            "<class 'numpy.uint16'> 27.586206896551722% smaller \n",
            "<class 'numpy.uint16'> 0.0% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  4.784327468734067e-16\n"
          ]
        }
      ],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class Linear_quant():\n",
        "    def __init__(self, quantization_type = np.uint32):\n",
        "        np.random.seed(786)\n",
        "        self.quantization_type = quantization_type\n",
        "        #quantization_level from 0 to 100\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self,data):\n",
        "        # data = data.astype(int)\n",
        "        self.xp = [data.min(), data.max()]\n",
        "        min = np.iinfo(self.quantization_type).min\n",
        "        max = np.iinfo(self.quantization_type).max\n",
        "\n",
        "        self.fp = [min, max]\n",
        "        enc = np.interp(data, self.xp, self.fp)\n",
        "        enc = enc.astype(self.quantization_type)\n",
        "        # encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quantization_level] \n",
        "        # result, encimg = cv2.imencode('.jpg', img, encode_param)\n",
        "\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        dec = np.interp(enc, self.fp, self.xp)\n",
        "        return dec\n",
        "    \n",
        "\n",
        "class customFreq_2_singleDCT():\n",
        "    # def __init__(self, precision_levels = [8, 4, 2]):\n",
        "    def __init__(self, precision_levels = [32, 16, 8]):\n",
        "        np.random.seed(786)\n",
        "        self.precision_levels = precision_levels\n",
        "        self.linearQuant = Linear_quant()\n",
        "        #quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(x, axis=-1, norm='ortho')\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(x, axis=-1, norm='ortho')\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        # Quantize frequency components\n",
        "        enc = self.linearQuant.encode(weights_f)\n",
        "        # enc = enc.astype(np.float16)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        weights_quantized = self.linearQuant.decode(enc)\n",
        "        weights_quantized = self.inverse_dct_transform(weights_quantized)\n",
        "        return weights_quantized\n",
        "    \n",
        "    \n",
        "data = list(np.random.rand(1,16).flatten())\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customFreq_2_singleDCT()\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Freq 2 (CustomFreq + Custom2) \n",
        "- best version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7566816  0.3074451  0.8939189  0.3866898  0.87385464 0.07623586\n",
            " 0.3765722  0.32655403 0.9408179  0.48312193 0.55546725 0.334706\n",
            " 0.28331634 0.5110989  0.5594255  0.6458654 ]\n",
            "Encoded [3822393318 1245889824 4294967295 2175506503 3614236915 1081460852\n",
            " 1084719233 1728903523 3366925843 1667425989  726524250  469755745\n",
            "          0 1282924090  676045071  397722513]\n",
            "After Quantization:  [0.75668158 0.30744515 0.89391888 0.38668986 0.87385463 0.07623591\n",
            " 0.37657221 0.32655406 0.94081779 0.48312199 0.55546728 0.33470603\n",
            " 0.28331629 0.51109892 0.55942545 0.64586541]\n",
            "<class 'numpy.uint16'> 27.586206896551722% smaller \n",
            "<class 'numpy.uint16'> 0.0% smaller \n",
            "enc shape:  (16,)\n",
            "dec shape:  (16,)\n",
            "mse:  1.7853008299711994e-15\n"
          ]
        }
      ],
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "\n",
        "class Linear_quant():\n",
        "    def __init__(self, quantization_type = np.uint32):\n",
        "        np.random.seed(786)\n",
        "        self.quantization_type = quantization_type\n",
        "        #quantization_level from 0 to 100\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self,data):\n",
        "        # data = data.astype(int)\n",
        "        self.xp = [data.min(), data.max()]\n",
        "        min = np.iinfo(self.quantization_type).min\n",
        "        max = np.iinfo(self.quantization_type).max\n",
        "\n",
        "        self.fp = [min, max]\n",
        "        enc = np.interp(data, self.xp, self.fp)\n",
        "        enc = enc.astype(self.quantization_type)\n",
        "        # encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), self.quantization_level] \n",
        "        # result, encimg = cv2.imencode('.jpg', img, encode_param)\n",
        "\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        dec = np.interp(enc, self.fp, self.xp)\n",
        "        return dec\n",
        "    \n",
        "\n",
        "class customFreq_2():\n",
        "    # def __init__(self, precision_levels = [8, 4, 2]):\n",
        "    def __init__(self, precision_levels = [32, 16, 8]):\n",
        "        np.random.seed(786)\n",
        "        self.precision_levels = precision_levels\n",
        "        self.linearQuant = Linear_quant()\n",
        "        #quantization_level from 0 to 100        \n",
        "\n",
        "    def dct_transform(self, x):\n",
        "        return dct(dct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def inverse_dct_transform(self, x):\n",
        "        return idct(idct(x, axis=-1, norm='ortho'), axis=-1, norm='ortho')\n",
        "\n",
        "    def quantize(self, x, precision):\n",
        "        scale = 2 ** (precision - 1) - 1\n",
        "        return np.round(x * scale) / scale\n",
        "\n",
        "\n",
        "    def encode(self, weights):\n",
        "        # Apply DCT to transform weights to the frequency domain\n",
        "        weights_f = self.dct_transform(weights)\n",
        "        # Quantize frequency components\n",
        "        enc = self.linearQuant.encode(weights_f)\n",
        "        # enc = enc.astype(np.float16)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, enc) :\n",
        "        weights_quantized = self.linearQuant.decode(enc)\n",
        "        weights_quantized = self.inverse_dct_transform(weights_quantized)\n",
        "        return weights_quantized\n",
        "    \n",
        "    \n",
        "data = list(np.random.rand(1,16).flatten())\n",
        "data = np.array(data, dtype = np.float32)\n",
        "\n",
        "print(data)\n",
        "data_type = np.uint16\n",
        "cmp = customFreq_2()\n",
        "enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "print(\"Encoded\",enc)\n",
        "\n",
        "dec = cmp.decode(enc)\n",
        "print(\"After Quantization: \", dec)\n",
        "\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "print(f\"{data_type} {100 * (sys.getsizeof(data)-sys.getsizeof(enc))/sys.getsizeof(data)}% smaller \")\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# from scipy.spatial.distance import mse\n",
        "print(\"enc shape: \", enc.shape)\n",
        "\n",
        "print(\"dec shape: \", dec.shape)\n",
        "\n",
        "print(\"mse: \",mse(dec.flatten(), data.flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.68138504 0.27689123 0.7873309  0.3404702  0.67810565 0.7277002\n",
            " 0.16247286 0.21167086 0.28807932 0.33239865 0.6241337  0.521482\n",
            " 0.8878545  0.532759   0.20141743 0.694045  ]\n",
            "Encoded [3072466507  677468423 3699769537 1053917931 3053049330 3346697295\n",
            "          0  291300180  743712833 1006126527 2733482738 2125684408\n",
            " 4294967295 2192455371  230589872 3147425865]\n",
            "After Quantization:  [0.68138504 0.27689123 0.78733093 0.34047019 0.67810565 0.72770017\n",
            " 0.16247286 0.21167086 0.28807932 0.33239865 0.62413371 0.52148199\n",
            " 0.88785452 0.53275901 0.20141743 0.69404501]\n",
            "<class 'numpy.uint16'> 27.586206896551722% smaller \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.27586206896551724"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_efficiency(data_size, cmp):\n",
        "    data = list(np.random.rand(data_size[0],data_size[1]).flatten())\n",
        "    data = np.array(data, dtype = np.float32)\n",
        "\n",
        "    print(data)\n",
        "    data_type = np.uint16\n",
        "\n",
        "    enc = cmp.encode(data) #quantization_level from 0 to 100\n",
        "    print(\"Encoded\",enc)\n",
        "\n",
        "    dec = cmp.decode(enc)\n",
        "    print(\"After Quantization: \", dec)\n",
        "\n",
        "    print(f\"{data_type} {100 * (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)}% smaller \")\n",
        "\n",
        "    eff = (sys.getsizeof(dec)-sys.getsizeof(enc))/sys.getsizeof(dec)\n",
        "\n",
        "\n",
        "    return eff\n",
        "\n",
        "# cmp = customFreq_2()\n",
        "\n",
        "# cmp = FP8()\n",
        "# cmp = QSGD(2,True)\n",
        "cmp = Custom_linear()\n",
        "\n",
        "efficiency = get_efficiency((1,16),cmp)\n",
        "efficiency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Federated Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% PAQ with Compression\n",
        "\n",
        "\n",
        "class MNIST_PAQ_COMP:\n",
        "\tdef __init__(self, cmp, filename=\"saved_models\", number_of_clients=1, aggregate_epochs=10, local_epochs=5, precision=7, r=1.0):\n",
        "\t\tnp.random.seed(786)\n",
        "\t\tself.model = None\n",
        "\t\tself.criterion = torch.nn.CrossEntropyLoss()\n",
        "\t\tself.optimizer = None\n",
        "\t\tself.number_of_clients = number_of_clients\n",
        "\t\tself.aggregate_epochs = aggregate_epochs\n",
        "\t\tself.local_epochs = local_epochs\n",
        "\t\tself.precision = precision\n",
        "\t\tself.r = r\n",
        "\t\tself.filename = filename\n",
        "\t\tself.compression_ratio = 0\n",
        "\t\tself.cmp=cmp\n",
        "\t\tself.cell_column_names = ['scheme_name','Epoch','Loss','Acc']\n",
        "\t\tself.cell = np.zeros((aggregate_epochs, len(self.cell_column_names)), dtype=object)\n",
        "\n",
        "\tdef define_model(self):\n",
        "\t\tself.model = torch.nn.Sequential(\n",
        "\t\t\ttorch.nn.Conv2d(1, 2, kernel_size=5),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Conv2d(2, 4, kernel_size=7),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Flatten(),\n",
        "\t\t\ttorch.nn.Linear(1296, 512),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Linear(512, 128),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Linear(128, 32),\n",
        "\t\t\ttorch.nn.ReLU(),\n",
        "\t\t\ttorch.nn.Linear(32, 10),\n",
        "\t\t\ttorch.nn.Softmax(dim=1),\n",
        "\t\t)\t\t\n",
        "\n",
        "\tdef get_weights_custom(self, dtype=np.float32):\n",
        "        \n",
        "\t\tprecision = self.precision \n",
        "\t\tweights = []\n",
        "\t\tstart_size = 0\n",
        "\t\tend_size = 0\n",
        "        \n",
        "        \n",
        "\t\tfor layer in self.model:\t\t\t\n",
        "\t\t\ttry:\n",
        "\t\t\t\tlayer_weights = layer.weight.detach().numpy().astype(dtype)\n",
        "\t\t\t\tstart_size = start_size + sys.getsizeof(layer_weights)\n",
        "\t\t\t\t# layer_weights_enc = self.cmp.encode(layer_weights)\n",
        "\t\t\t\tlayer_weights_enc = self.cmp.encode(layer_weights.flatten())\n",
        "\t\t\t\tdecoded_weights = self.cmp.decode(layer_weights_enc)\n",
        "\t\t\t\tdecoded_weights = decoded_weights.astype(dtype)\n",
        "\t\t\t\tdecoded_weights = decoded_weights.reshape(layer_weights.shape)\n",
        "\t\t\t\tend_size = end_size + sys.getsizeof(decoded_weights)\n",
        "\t\t\n",
        "\t\t\t\tlayer_bias = layer.bias.detach().numpy().astype(dtype)\n",
        "\t\t\t\t# layer_bias_enc = self.cmp.encode(layer_bias)\t\n",
        "\t\t\t\tlayer_bias_enc = self.cmp.encode(layer_bias.flatten())\t\t\t\n",
        "\t\t\t\tdecoded_bias = self.cmp.decode(layer_bias_enc)\n",
        "\t\t\t\tdecoded_bias = decoded_bias.astype(dtype)\n",
        "\t\t\t\tdecoded_bias = decoded_bias.reshape(layer_bias.shape)\n",
        "\t\t\t\tweights.append([decoded_weights,decoded_bias])\n",
        "\t\t\t\t\n",
        "\t\t\texcept:\n",
        "\t\t\t\tcontinue\n",
        "\t\tself.compression_ratio = ((start_size-end_size)/start_size)*100\n",
        "\t\treturn np.array(weights),start_size,end_size\n",
        "\n",
        "\n",
        "\n",
        "\tdef set_weights(self, weights):\n",
        "\t\tindex = 0\n",
        "\t\tfor layer_no, layer in enumerate(self.model):\n",
        "\t\t\ttry:\n",
        "\t\t\t\t_ = self.model[layer_no].weight\n",
        "\t\t\t\tself.model[layer_no].weight = torch.nn.Parameter(weights[index][0])\n",
        "\t\t\t\tself.model[layer_no].bias = torch.nn.Parameter(weights[index][1])\n",
        "\t\t\t\tindex += 1\n",
        "\t\t\texcept:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\tdef average_weights(self, all_weights):\n",
        "        \n",
        "\t\tall_weights = np.array(all_weights)\n",
        "\t\tall_weights = np.mean(all_weights, axis=0)\n",
        "\t\tall_weights = [[torch.from_numpy(i[0].astype(np.float32)), torch.from_numpy(i[1].astype(np.float32))] for i in all_weights]\n",
        "\t\treturn all_weights\n",
        "\n",
        "\n",
        "\n",
        "\tdef client_generator(self, train_x, train_y):\n",
        "\t\tnumber_of_clients = self.number_of_clients\n",
        "\t\tsize = train_y.shape[0]//number_of_clients\n",
        "\t\ttrain_x, train_y = train_x.numpy(), train_y.numpy()\n",
        "\t\ttrain_x = np.array([train_x[i:i+size] for i in range(0, len(train_x)-len(train_x)%size, size)])\n",
        "\t\ttrain_y = np.array([train_y[i:i+size] for i in range(0, len(train_y)-len(train_y)%size, size)])\n",
        "\t\ttrain_x = torch.from_numpy(train_x)\n",
        "\t\ttrain_y = torch.from_numpy(train_y)\n",
        "\t\treturn train_x, train_y\n",
        "\n",
        "\tdef single_client(self, dataset, weights, E):\n",
        "\t\tself.define_model()\n",
        "\t\tif weights is not None:\n",
        "\t\t\tself.set_weights(weights)\n",
        "\t\tself.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\t\tfor epoch in range(E):\n",
        "\t\t\trunning_loss = 0\n",
        "\t\t\tfor batch_x, target in zip(dataset['x'], dataset['y']):\n",
        "\t\t\t\toutput = self.model(batch_x)\n",
        "\t\t\t\tloss = self.criterion(output, target)\n",
        "\t\t\t\tself.optimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\tself.optimizer.step()\n",
        "\t\t\t\trunning_loss += loss.item()\n",
        "\t\t\trunning_loss /= len(dataset['y'])\n",
        "\t\t# weights,start_size,end_size = self.get_weights()\n",
        "\t\tweights,start_size,end_size = self.get_weights_custom()\n",
        "\t\t\n",
        "\t\treturn weights, running_loss, start_size,end_size\n",
        "\tdef test_aggregated_model(self, test_x, test_y, epoch):\n",
        "\t\tacc = 0\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor batch_x, batch_y in zip(test_x, test_y):\n",
        "\t\t\t\ty_pred = self.model(batch_x)\n",
        "\t\t\t\ty_pred = torch.argmax(y_pred, dim=1)\n",
        "\t\t\t\tacc += torch.sum(y_pred == batch_y)/y_pred.shape[0]\n",
        "\t\ttorch.save(self.model, \"./\"+self.filename+\"/model_epoch_\"+str(epoch+1)+\".pt\")\n",
        "\t\treturn (acc/test_x.shape[0])\n",
        "\t\t\t\n",
        "\n",
        "\tdef train_aggregator(self, datasets, datasets_test):\n",
        "\t\tlocal_epochs = self.local_epochs\n",
        "\t\taggregate_epochs = self.aggregate_epochs\n",
        "\t\tos.system('mkdir '+self.filename)\n",
        "\t\tE = local_epochs\n",
        "\t\taggregate_weights = None\n",
        "\t\tfor epoch in range(aggregate_epochs):\n",
        "\t\t\tall_weights = []\n",
        "\t\t\tclient = 0\n",
        "\t\t\trunning_loss = 0\n",
        "\t\t\tselections = np.arange(datasets['x'].shape[0])\n",
        "\t\t\tnp.random.shuffle(selections)\n",
        "\t\t\tselections = selections[:int(self.r*datasets['x'].shape[0])]\n",
        "\t\t\tclients = tqdm(zip(datasets['x'][selections], datasets['y'][selections]), total=selections.shape[0])\n",
        "\t\t\tfor dataset_x, dataset_y in clients:\n",
        "\t\t\t\tdataset = {'x':dataset_x, 'y':dataset_y}\n",
        "\t\t\t\tweights, loss, start_size,end_size = self.single_client(dataset, aggregate_weights, E)\n",
        "\t\t\t\trunning_loss += loss\n",
        "\t\t\t\tall_weights.append(weights)\n",
        "\t\t\t\tclient += 1\n",
        "\t\t\t\tclients.set_description(str({\"Epoch\":epoch+1,\"Loss\": round(running_loss/client, 5)}))\n",
        "\t\t\t\tclients.refresh()\n",
        "\t\t\taggregate_weights = self.average_weights(all_weights)\n",
        "\t\t\tagg_weight = self.set_weights(aggregate_weights)\n",
        "\t\t\ttest_acc = self.test_aggregated_model(datasets_test['x'], datasets_test['y'], epoch)\n",
        "\t\t\tprint(\"Test Accuracy:\", round(test_acc.item(), 5))\n",
        "\t\t\tprint(\"Compression Ratio \", self.compression_ratio)\n",
        "\t\t\t# print()\n",
        "\t\t\tself.cell[epoch][0]=self.cmp.__class__.__name__\n",
        "\t\t\tself.cell[epoch][1]=epoch\n",
        "\t\t\tself.cell[epoch][2]= round(running_loss/client, 3)\n",
        "\t\t\tself.cell[epoch][3]= round(test_acc.item(), 3)\n",
        "\n",
        "\t\t\tprint(f'start_size: {start_size/1024},end_size: {end_size}')\n",
        "\t\t\tclients.close()\n",
        "\t\treturn self.cell\n",
        "\n",
        "#%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/164 [00:00<?, ?it/s]C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_17416\\592980940.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(weights),start_size,end_size\n",
            "{'Epoch': 1, 'Loss': 2.30095}: 100%|██████████| 164/164 [00:28<00:00,  5.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.11375\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 2, 'Loss': 2.07821}: 100%|██████████| 164/164 [00:30<00:00,  5.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.39453\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 3, 'Loss': 1.8915}: 100%|██████████| 164/164 [00:28<00:00,  5.72it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5968\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 4, 'Loss': 1.82647}: 100%|██████████| 164/164 [00:30<00:00,  5.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6403\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 5, 'Loss': 1.80333}: 100%|██████████| 164/164 [00:31<00:00,  5.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.65618\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 6, 'Loss': 1.78703}: 100%|██████████| 164/164 [00:30<00:00,  5.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.66331\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 7, 'Loss': 1.778}: 100%|██████████| 164/164 [00:28<00:00,  5.80it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.66822\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 8, 'Loss': 1.71688}: 100%|██████████| 164/164 [00:28<00:00,  5.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6717\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 9, 'Loss': 1.62058}: 100%|██████████| 164/164 [00:28<00:00,  5.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.84328\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'Epoch': 10, 'Loss': 1.59803}: 100%|██████████| 164/164 [00:28<00:00,  5.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.85253\n",
            "Compression Ratio  99.97330216770052\n",
            "start_size: 2867.7421875,end_size: 784\n",
            "Time Taken:  309.1846697330475\n"
          ]
        }
      ],
      "source": [
        "number_of_clients = 328\n",
        "aggregate_epochs = 10\n",
        "local_epochs = 3\n",
        "r = 0.5\n",
        "current_time = datetime.now().time()\n",
        "epoch_time = time.time()\n",
        "tic = time.time()\n",
        "filename = f\"saved_models_{epoch_time}\"\n",
        "train_x, train_y, test_x, test_y = Dataset().load_csv()\n",
        "# cmp = customFreq_2()\n",
        "\n",
        "# cmp = FP8()\n",
        "# cmp = QSGD(2,True)\n",
        "# cmp = Custom_linear_16()\n",
        "cmp = customFreq_2_singleDCT()\n",
        "m_8 = MNIST_PAQ_COMP(cmp = cmp, filename=filename, r=r, number_of_clients=number_of_clients, aggregate_epochs=aggregate_epochs, local_epochs=local_epochs)\n",
        "train_x, train_y = m_8.client_generator(train_x, train_y)\n",
        "result = m_8.train_aggregator({'x':train_x, 'y':train_y}, {'x':test_x, 'y':test_y})\n",
        "print(\"Time Taken: \", time.time()-tic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scheme_name</th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>0</td>\n",
              "      <td>2.301</td>\n",
              "      <td>0.114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>1</td>\n",
              "      <td>2.078</td>\n",
              "      <td>0.395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>2</td>\n",
              "      <td>1.891</td>\n",
              "      <td>0.597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>3</td>\n",
              "      <td>1.826</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>4</td>\n",
              "      <td>1.803</td>\n",
              "      <td>0.656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>5</td>\n",
              "      <td>1.787</td>\n",
              "      <td>0.663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>6</td>\n",
              "      <td>1.778</td>\n",
              "      <td>0.668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>7</td>\n",
              "      <td>1.717</td>\n",
              "      <td>0.672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>8</td>\n",
              "      <td>1.621</td>\n",
              "      <td>0.843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>customFreq_2_singleDCT</td>\n",
              "      <td>9</td>\n",
              "      <td>1.598</td>\n",
              "      <td>0.853</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              scheme_name Epoch   Loss    Acc\n",
              "0  customFreq_2_singleDCT     0  2.301  0.114\n",
              "1  customFreq_2_singleDCT     1  2.078  0.395\n",
              "2  customFreq_2_singleDCT     2  1.891  0.597\n",
              "3  customFreq_2_singleDCT     3  1.826   0.64\n",
              "4  customFreq_2_singleDCT     4  1.803  0.656\n",
              "5  customFreq_2_singleDCT     5  1.787  0.663\n",
              "6  customFreq_2_singleDCT     6  1.778  0.668\n",
              "7  customFreq_2_singleDCT     7  1.717  0.672\n",
              "8  customFreq_2_singleDCT     8  1.621  0.843\n",
              "9  customFreq_2_singleDCT     9  1.598  0.853"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result =  pd.DataFrame(result, columns=m_8.cell_column_names)\n",
        "file_path_1 = r'E:\\MS_Thesis\\Pre-defense-june\\Results_for_update\\proposed_variations.csv'\n",
        "display(result)\n",
        "if os.path.exists(file_path_1):\n",
        "    # Read the existing CSV file\n",
        "    existing_data = pd.read_csv(file_path_1)\n",
        "    \n",
        "    # Identify the last epoch in the current run\n",
        "    last_epoch = result['Epoch'].max()\n",
        "    \n",
        "    # Iterate over the new results and update the existing ones if necessary\n",
        "    for index, row in result.iterrows():\n",
        "        scheme_name = row['scheme_name']\n",
        "        epoch = row['Epoch']\n",
        "        new_acc = row['Acc']\n",
        "        \n",
        "        # Check if the scheme_name and epoch already exist in the existing data\n",
        "        existing_entry = existing_data[(existing_data['scheme_name'] == scheme_name) & (existing_data['Epoch'] == epoch)]\n",
        "        \n",
        "        if not existing_entry.empty:\n",
        "            # Check if the new accuracy at the last epoch is greater than the existing one\n",
        "            if epoch == last_epoch:\n",
        "                existing_last_epoch_entry = existing_data[(existing_data['scheme_name'] == scheme_name) & (existing_data['Epoch'] == last_epoch)]\n",
        "                if new_acc > existing_last_epoch_entry['Acc'].values[0]:\n",
        "                    # Update the existing entry\n",
        "                    existing_data.loc[(existing_data['scheme_name'] == scheme_name) & (existing_data['Epoch'] == epoch), ['Loss', 'Acc']] = row[['Loss', 'Acc']]\n",
        "        else:\n",
        "            # Append the new row to the existing data\n",
        "            # existing_data = existing_data.append(row, ignore_index=True)\n",
        "            existing_data = pd.concat([existing_data, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    # Save the updated data back to the CSV file\n",
        "    existing_data.to_csv(file_path_1, index=False)\n",
        "else:\n",
        "    # If the file does not exist, save the new result as a new CSV file\n",
        "    result.to_csv(file_path_1, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scheme_name</th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>0</td>\n",
              "      <td>2.301</td>\n",
              "      <td>0.103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>1</td>\n",
              "      <td>2.046</td>\n",
              "      <td>0.443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>2</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>3</td>\n",
              "      <td>1.83</td>\n",
              "      <td>0.631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>4</td>\n",
              "      <td>1.717</td>\n",
              "      <td>0.676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>5</td>\n",
              "      <td>1.643</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>6</td>\n",
              "      <td>1.629</td>\n",
              "      <td>0.832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>7</td>\n",
              "      <td>1.623</td>\n",
              "      <td>0.838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>8</td>\n",
              "      <td>1.62</td>\n",
              "      <td>0.839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Custom_linear</td>\n",
              "      <td>9</td>\n",
              "      <td>1.615</td>\n",
              "      <td>0.842</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     scheme_name Epoch   Loss    Acc\n",
              "0  Custom_linear     0  2.301  0.103\n",
              "1  Custom_linear     1  2.046  0.443\n",
              "2  Custom_linear     2   1.88  0.604\n",
              "3  Custom_linear     3   1.83  0.631\n",
              "4  Custom_linear     4  1.717  0.676\n",
              "5  Custom_linear     5  1.643   0.82\n",
              "6  Custom_linear     6  1.629  0.832\n",
              "7  Custom_linear     7  1.623  0.838\n",
              "8  Custom_linear     8   1.62  0.839\n",
              "9  Custom_linear     9  1.615  0.842"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
