{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\FedPAQ_env\\lib\\site-packages\\julia\\juliainfo.py:93: UserWarning: julia warned:\n",
      "The latest version of Julia in the `release` channel is 1.11.1+0.x64.w64.mingw32. You currently have `1.10.5+0.x64.w64.mingw32` installed. Run:\n",
      "\n",
      "  juliaup update\n",
      "\n",
      "in your terminal shell to install Julia 1.11.1+0.x64.w64.mingw32 and update the `release` channel to that version.\n",
      "  warnings.warn(\"{} warned:\\n{}\".format(julia, stderr))\n"
     ]
    }
   ],
   "source": [
    "import schemes as sc\n",
    "from model_cifar import CIFAR_PAQ_COMP,CIFAR_PAQ_COMP_ADAPTIVE \n",
    "\n",
    "\n",
    "\n",
    "from load_dataset import Dataset\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Cifar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cifar_location = r'D:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\cifar-10\\cifar-10-python'\n",
    "\n",
    "class CIFAR10Dataset:\n",
    "    def __init__(self, iid=True, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.iid = iid\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    def load_data(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        # Download and load the training data\n",
    "        trainset = torchvision.datasets.CIFAR10(root=cifar_location, train=True, download=False, transform=transform)\n",
    "        \n",
    "        # Download and load the test data\n",
    "        testset = torchvision.datasets.CIFAR10(root=cifar_location, train=False, download=False, transform=transform)\n",
    "\n",
    "        # Calculate number of batches\n",
    "        num_batches = len(trainset.data) // self.batch_size\n",
    "\n",
    "        # Truncate the dataset to a multiple of the batch size\n",
    "        train_x = trainset.data[:num_batches * self.batch_size]\n",
    "        train_y = np.array(trainset.targets[:num_batches * self.batch_size])\n",
    "\n",
    "        # Reshape into batches\n",
    "        train_x_batches = train_x.reshape(num_batches, self.batch_size, *train_x.shape[1:])\n",
    "        train_y_batches = train_y.reshape(num_batches, self.batch_size)\n",
    "        \n",
    "        return trainset.targets, trainset.data, testset.targets, testset.data\n",
    "\n",
    "    def load_data_2(self):\n",
    "        batch_size = self.batch_size\n",
    "        trainset = torchvision.datasets.CIFAR10(root=cifar_location, train=True, download=False)\n",
    "        testset = torchvision.datasets.CIFAR10(root=cifar_location, train=False, download=False)\n",
    "\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = trainset.data, testset.data, trainset.targets, testset.targets\n",
    "\n",
    "        self.train_x = np.array([self.train_x[n:n+batch_size] for n in range(0, len(self.train_x)-batch_size, batch_size)])/255.0\n",
    "        self.test_x = np.array([self.test_x[n:n+batch_size] for n in range(0, len(self.test_x)-batch_size, batch_size)])/255.0\n",
    "        self.train_y = np.array([self.train_y[n:n+batch_size] for n in range(0, len(self.train_y)-batch_size, batch_size)])\n",
    "        self.test_y = np.array([self.test_y[n:n+batch_size] for n in range(0, len(self.test_y)-batch_size, batch_size)])\n",
    "\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = torch.from_numpy(self.train_x), torch.from_numpy(self.train_y), torch.from_numpy(self.test_x), torch.from_numpy(self.test_y)\n",
    "\n",
    "        return self.train_x, self.train_y, self.test_x, self.test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Code using Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_43628\\3063256684.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_y = torch.tensor(train_y, dtype=torch.long)\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_43628\\3063256684.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_y = torch.tensor(test_y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([390, 128, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:00<?, ?it/s]d:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\FedPAQ_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\FedPAQ_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "{'Epoch': 1, 'Loss': 0.59534}:  16%|█▋        | 32/195 [09:09<47:24, 17.45s/it] "
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "deallocated bytearray object has exported buffers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mSystemError\u001b[0m: deallocated bytearray object has exported buffers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 1, 'Loss': 0.5867}:  17%|█▋        | 33/195 [09:23<43:58, 16.29s/it] "
     ]
    }
   ],
   "source": [
    "number_of_clients = 390\n",
    "aggregate_epochs = 10\n",
    "local_epochs = 3\n",
    "r = 0.5\n",
    "\n",
    "current_time = datetime.now().time()\n",
    "epoch_time = time.time()\n",
    "tic = time.time()\n",
    "filename = f\"saved_models_{epoch_time}\"\n",
    "\n",
    "\n",
    "\n",
    "dataset = CIFAR10Dataset(iid=True, batch_size=128)\n",
    "train_x, train_y, test_x, test_y = dataset.load_data_2()\n",
    "train_x = train_x.permute(0, 1, 4, 2, 3)\n",
    "test_x = test_x.permute(0, 1, 4, 2, 3)\n",
    "\n",
    "train_x = train_x.float()\n",
    "test_x = test_x.float()\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)\n",
    "print(train_x.shape)\n",
    "cmp = sc.FP8()\n",
    "\n",
    "c_8 = CIFAR_PAQ_COMP(cmp = cmp, filename=filename, r=r, number_of_clients=number_of_clients, aggregate_epochs=aggregate_epochs, local_epochs=local_epochs)\n",
    "train_x, train_y = c_8.client_generator(train_x, train_y)\n",
    "result = c_8.train_aggregator({'x':train_x, 'y':train_y}, {'x':test_x, 'y':test_y})\n",
    "print(\"Time Taken: \", time.time()-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([390, 1, 128, 3, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "predicted_labels = torch.argmax(output, dim=1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = r'E:\\MS_Thesis\\Pre-defense-june\\Results_for_update\\proposed_variations.csv'\n",
    "\n",
    "\n",
    "result =  pd.DataFrame(result, columns=m_8.cell_column_names)\n",
    "display(result)\n",
    "if os.path.exists(file_path_1):\n",
    "    # Read the existing CSV file\n",
    "    existing_data = pd.read_csv(file_path_1)\n",
    "    \n",
    "    # Identify the last epoch in the current run\n",
    "    last_epoch = result['Epoch'].max()\n",
    "    \n",
    "    # Iterate over the new results and update the existing ones if necessary\n",
    "    for index, row in result.iterrows():\n",
    "        scheme_name = row['scheme_name']\n",
    "        epoch = row['Epoch']\n",
    "        new_acc = row['Acc']\n",
    "        \n",
    "        # Check if the scheme_name and epoch already exist in the existing data\n",
    "        existing_entry = existing_data[(existing_data['scheme_name'] == scheme_name) & (existing_data['Epoch'] == epoch)]\n",
    "        \n",
    "        if not existing_entry.empty:\n",
    "            # Check if the new accuracy at the last epoch is greater than the existing one\n",
    "            if epoch == last_epoch:\n",
    "                existing_last_epoch_entry = existing_data[(existing_data['scheme_name'] == scheme_name) & (existing_data['Epoch'] == last_epoch)]\n",
    "                if new_acc > existing_last_epoch_entry['Acc'].values[0]:\n",
    "                    # Update the existing entry\n",
    "                    existing_data.loc[(existing_data['scheme_name'] == scheme_name) & (existing_data['Epoch'] == epoch), ['Loss', 'Acc']] = row[['Loss', 'Acc']]\n",
    "        else:\n",
    "            # Append the new row to the existing data\n",
    "            # existing_data = existing_data.append(row, ignore_index=True)\n",
    "            existing_data = pd.concat([existing_data, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    # Save the updated data back to the CSV file\n",
    "    existing_data.to_csv(file_path_1, index=False)\n",
    "else:\n",
    "    # If the file does not exist, save the new result as a new CSV file\n",
    "    result.to_csv(file_path_1, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<schemes.customFreq_2 object at 0x000002A4B1EDAEC0>: 0.275862069, <schemes.QSGD object at 0x000002A4B1F3B370>: 0.528846154, <schemes.Custom_linear_16 object at 0x000002A4B1EBF220>: 0.413793103}\n",
      "dict_keys([<schemes.QSGD object at 0x000002A4B1F3B370>, <schemes.Custom_linear_16 object at 0x000002A4B1EBF220>, <schemes.customFreq_2 object at 0x000002A4B1EDAEC0>])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164 [00:00<?, ?it/s]e:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\model.py:247: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  weight_array = np.array(weights)\n",
      "e:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\model.py:254: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(weights),start_size,end_size\n",
      "{'Epoch': 1, 'Loss': 2.30105}: 100%|██████████| 164/164 [00:27<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.10106\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'Loss': 1.97014}: 100%|██████████| 164/164 [00:25<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.43947\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 3, 'Loss': 1.65861}: 100%|██████████| 164/164 [00:27<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.82852\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 4, 'Loss': 1.588}: 100%|██████████| 164/164 [00:27<00:00,  6.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.87923\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 5, 'Loss': 1.557}: 100%|██████████| 164/164 [00:26<00:00,  6.22it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.90127\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 6, 'Loss': 1.54008}: 100%|██████████| 164/164 [00:26<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.91897\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 7, 'Loss': 1.53749}: 100%|██████████| 164/164 [00:26<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.92524\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 8, 'Loss': 1.52304}: 100%|██████████| 164/164 [00:26<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.93191\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 9, 'Loss': 1.51883}: 100%|██████████| 164/164 [00:26<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.94026\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 10, 'Loss': 1.51467}: 100%|██████████| 164/164 [00:27<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.93872\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n",
      "Time Taken:  0:04:42.358223\n"
     ]
    }
   ],
   "source": [
    "number_of_clients = 328\n",
    "aggregate_epochs = 10\n",
    "local_epochs = 3\n",
    "r = 0.5\n",
    "\n",
    "\n",
    "current_time = datetime.now().time()\n",
    "tic = datetime.now()\n",
    "filename = \"saved_models_x\"\n",
    "train_x, train_y, test_x, test_y = Dataset().load_csv()\n",
    "\n",
    "cmp_list = [sc.customFreq_2(),sc.QSGD(2,True),sc.Custom_linear_16()]\n",
    "d = {}\n",
    "\n",
    "for cmp in cmp_list:\n",
    "    name, acc, time, eff, combined = sc.get_time_efficiency_acc(cmp)\n",
    "    d[cmp] = eff\n",
    "    \n",
    "print(d)\n",
    "sorted_schemes = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_schemes.keys())\n",
    "\n",
    "m_8 = MNIST_PAQ_COMP_ADAPTIVE(cmp_list=list(sorted_schemes.keys()), filename=filename, r=r, number_of_clients=number_of_clients, aggregate_epochs=aggregate_epochs, local_epochs=local_epochs)\n",
    "train_x, train_y = m_8.client_generator(train_x, train_y)\n",
    "agg_weights = m_8.train_aggregator({'x':train_x, 'y':train_y}, {'x':test_x, 'y':test_y})\n",
    "print(\"Time Taken: \", datetime.now()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m d \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cmp \u001b[38;5;129;01min\u001b[39;00m cmp_list:\n\u001b[1;32m---> 19\u001b[0m     name, acc, time, eff, combined \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_time_efficiency_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     d[cmp] \u001b[38;5;241m=\u001b[39m eff\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n",
      "File \u001b[1;32md:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\schemes.py:429\u001b[0m, in \u001b[0;36mget_time_efficiency_acc\u001b[1;34m(cmp, acc_loss_file, time_eff_file)\u001b[0m\n\u001b[0;32m    420\u001b[0m df_properties_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined_Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    421\u001b[0m     weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m df_properties_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    422\u001b[0m     weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m df_properties_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m  \n\u001b[0;32m    423\u001b[0m     weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEfficiency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m df_properties_scaled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEfficiency\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    424\u001b[0m )\n\u001b[0;32m    427\u001b[0m name \u001b[38;5;241m=\u001b[39m cmp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 429\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[43mdf_properties\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_properties\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheme_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    430\u001b[0m acc \u001b[38;5;241m=\u001b[39m df_properties[df_properties[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheme_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mname][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    431\u001b[0m eff \u001b[38;5;241m=\u001b[39m df_properties[df_properties[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheme_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mname][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEfficiency\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "number_of_clients = 328\n",
    "aggregate_epochs = 10\n",
    "local_epochs = 3\n",
    "r = 0.5\n",
    "\n",
    "\n",
    "current_time = datetime.now().time()\n",
    "tic = datetime.now()\n",
    "filename = \"saved_models_x\"\n",
    "train_x, train_y, test_x, test_y = Dataset().load_csv()\n",
    "\n",
    "# cmp_list = [sc.customFreq_2(),sc.QSGD(2,True),sc.Custom_linear_16()]\n",
    "# cmp_list = [sc.customFreq_2(),sc.customFreq_2(),sc.customFreq_2()]\n",
    "cmp_list = [sc.customFreq_1_5(),sc.customFreq_1_5(),sc.customFreq_1_5()]\n",
    "\n",
    "d = {}\n",
    "\n",
    "for cmp in cmp_list:\n",
    "    name, acc, time, eff, combined = sc.get_time_efficiency_acc(cmp)\n",
    "    d[cmp] = eff\n",
    "    \n",
    "print(d)\n",
    "sorted_schemes = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_schemes.keys())\n",
    "\n",
    "m_8 = MNIST_PAQ_COMP_ADAPTIVE(cmp_list=list(sorted_schemes.keys()), filename=filename, r=r, number_of_clients=number_of_clients, aggregate_epochs=aggregate_epochs, local_epochs=local_epochs)\n",
    "train_x, train_y = m_8.client_generator(train_x, train_y)\n",
    "agg_weights,client_details = m_8.train_aggregator({'x':train_x, 'y':train_y}, {'x':test_x, 'y':test_y},logging=True)\n",
    "print(\"Time Taken: \", datetime.now()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency Acc: 0.16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Convert dictionary to DataFrame\n",
    "df = pd.DataFrame.from_dict(client_details, orient='index')\n",
    "\n",
    "# Step 2: Rename the columns\n",
    "df.columns = ['Bandwidth', 'Traffic', 'Latency', 'Compression']\n",
    "\n",
    "# Step 3: Save to CSV file\n",
    "df.to_csv(r'D:\\MS_Thesis\\Pre-defense-june\\Results_for_update\\client_latency_data_customFreq_1_5.csv', index=True)\n",
    "\n",
    "# Optional: Print the head of the DataFrame to verify\n",
    "average_latency = df['Latency'].mean()\n",
    "print(f'Average Latency Acc: {average_latency:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency\n",
    "customFreq_2 0.20\n",
    "Adaptive 0.13\n",
    "QSGD 0.12\n",
    "FP8 0.34\n",
    "Custom_linear 0.18\n",
    "Custom_linear_16 0.16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<schemes.customFreq_2 object at 0x0000016B2079DAE0>: 0.275862069, <schemes.QSGD object at 0x0000016B2079E0B0>: 0.528846154, <schemes.Custom_linear_16 object at 0x0000016B2079D8A0>: 0.413793103}\n",
      "dict_keys([<schemes.QSGD object at 0x0000016B2079E0B0>, <schemes.Custom_linear_16 object at 0x0000016B2079D8A0>, <schemes.customFreq_2 object at 0x0000016B2079DAE0>])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164 [00:00<?, ?it/s]d:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\model.py:249: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  weight_array = np.array(weights)\n",
      "d:\\MS_Thesis\\Hierarchical_quantization\\FedPAQ-MNIST-implemenation-main\\model.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(weights),start_size,end_size\n",
      "{'Epoch': 1, 'Loss': 2.30108}: 100%|██████████| 164/164 [00:20<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.09963\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'Loss': 1.98181}: 100%|██████████| 164/164 [00:20<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.40399\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 3, 'Loss': 1.70667}: 100%|██████████| 164/164 [00:20<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7763\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 4, 'Loss': 1.62547}: 100%|██████████| 164/164 [00:21<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.82934\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 5, 'Loss': 1.58647}: 100%|██████████| 164/164 [00:20<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.84475\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 6, 'Loss': 1.53287}: 100%|██████████| 164/164 [00:21<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.91553\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 7, 'Loss': 1.5175}: 100%|██████████| 164/164 [00:20<00:00,  7.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.93768\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 8, 'Loss': 1.50744}: 100%|██████████| 164/164 [00:20<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.94417\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 9, 'Loss': 1.5063}: 100%|██████████| 164/164 [00:20<00:00,  7.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9465\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'Epoch': 10, 'Loss': 1.50474}: 100%|██████████| 164/164 [00:20<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.94789\n",
      "Compression Ratio  99.97330216770052\n",
      "start_size: 2867.7421875,end_size: 784\n",
      "Time Taken:  0:03:42.532847\n",
      "Average Latency Acc: 0.12\n"
     ]
    }
   ],
   "source": [
    "number_of_clients = 328\n",
    "aggregate_epochs = 10\n",
    "local_epochs = 3\n",
    "r = 0.5\n",
    "\n",
    "\n",
    "current_time = datetime.now().time()\n",
    "tic = datetime.now()\n",
    "filename = \"saved_models_x\"\n",
    "train_x, train_y, test_x, test_y = Dataset().load_csv()\n",
    "\n",
    "cmp_list = [sc.customFreq_2(),sc.QSGD(2,True),sc.Custom_linear_16()]\n",
    "# cmp_list = [sc.customFreq_2(),sc.customFreq_2(),sc.customFreq_2()]\n",
    "# cmp_list = [sc.customFreq_1_5(),sc.customFreq_1_5(),sc.customFreq_1_5()]\n",
    "\n",
    "d = {}\n",
    "\n",
    "w_acc= 0.3\n",
    "w_time = 0.3\n",
    "w_eff = 0.2\n",
    "\n",
    "for cmp in cmp_list:\n",
    "    name, acc, time, eff, combined = sc.get_time_efficiency_acc(cmp, w_acc=w_acc, w_time=w_time, w_eff=w_eff)\n",
    "    d[cmp] = eff\n",
    "    \n",
    "print(d)\n",
    "sorted_schemes = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_schemes.keys())\n",
    "\n",
    "m_8 = MNIST_PAQ_COMP_ADAPTIVE(cmp_list=list(sorted_schemes.keys()), filename=filename, r=r, number_of_clients=number_of_clients, aggregate_epochs=aggregate_epochs, local_epochs=local_epochs)\n",
    "train_x, train_y = m_8.client_generator(train_x, train_y)\n",
    "agg_weights,client_details = m_8.train_aggregator({'x':train_x, 'y':train_y}, {'x':test_x, 'y':test_y},logging=True)\n",
    "print(\"Time Taken: \", datetime.now()-tic)\n",
    "\n",
    "\n",
    "# Step 1: Convert dictionary to DataFrame\n",
    "df = pd.DataFrame.from_dict(client_details, orient='index')\n",
    "\n",
    "# Step 2: Rename the columns\n",
    "df.columns = ['Bandwidth', 'Traffic', 'Latency', 'Compression']\n",
    "\n",
    "# Step 3: Save to CSV file\n",
    "df.to_csv(rf'D:\\MS_Thesis\\Pre-defense-june\\hyperparameter_tuning\\client_latency_data_w_acc{w_acc}_w_time_{w_time}_w_eff_{w_eff}.csv', index=True)\n",
    "\n",
    "# Optional: Print the head of the DataFrame to verify\n",
    "average_latency = df['Latency'].mean()\n",
    "print(f'Average Latency Acc: {average_latency:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
